<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Jose Parreno Garcia" />


<title>Linear Regression in R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="site_libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 52px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 57px;
  margin-top: -57px;
}

.section h2 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h3 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h4 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h5 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h6 {
  padding-top: 57px;
  margin-top: -57px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine learning linear regression</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Machine learning linear regression</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Linear Regression in R</h1>
<h4 class="author"><em>Jose Parreno Garcia</em></h4>
<h4 class="date"><em>November 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#understanding-regression"><span class="toc-section-number">1</span> Understanding regression</a><ul>
<li><a href="#simple-linear-regression"><span class="toc-section-number">1.1</span> Simple linear regression</a><ul>
<li><a href="#line-of-best-fit"><span class="toc-section-number">1.1.1</span> Line of best fit</a></li>
<li><a href="#correlations"><span class="toc-section-number">1.1.2</span> Correlations</a></li>
<li><a href="#first-simple-linear-model-and-coefficient-interpretation"><span class="toc-section-number">1.1.3</span> First simple linear model and coefficient interpretation</a></li>
</ul></li>
<li><a href="#multiple-linear-regression"><span class="toc-section-number">1.2</span> Multiple linear regression</a></li>
</ul></li>
<li><a href="#example---predicting-medical-expenses-using-linear-regression"><span class="toc-section-number">2</span> Example - predicting medical expenses using linear regression</a><ul>
<li><a href="#exploring-relationships-among-features---the-correlation-matrix"><span class="toc-section-number">2.1</span> Exploring relationships among features - the correlation matrix</a></li>
<li><a href="#visualizing-relationships-among-features---the-scatterplot-matrix"><span class="toc-section-number">2.2</span> Visualizing relationships among features - the scatterplot matrix</a></li>
<li><a href="#training-a-model-on-the-data"><span class="toc-section-number">2.3</span> Training a model on the data</a></li>
<li><a href="#evaluating-model-performance"><span class="toc-section-number">2.4</span> Evaluating model performance</a></li>
<li><a href="#improving-model-performance"><span class="toc-section-number">2.5</span> Improving model performance</a><ul>
<li><a href="#adding-non-linear-relationships"><span class="toc-section-number">2.5.1</span> Adding non-linear relationships</a></li>
<li><a href="#converting-a-numeric-variable-to-a-binary-indicator"><span class="toc-section-number">2.5.2</span> Converting a numeric variable to a binary indicator</a></li>
<li><a href="#adding-interaction-effects"><span class="toc-section-number">2.5.3</span> Adding interaction effects</a></li>
<li><a href="#putting-it-all-together---an-improved-regression-model"><span class="toc-section-number">2.5.4</span> Putting it all together - an improved regression model</a></li>
</ul></li>
</ul></li>
<li><a href="#machine-learning-theory"><span class="toc-section-number">3</span> Machine Learning Theory</a><ul>
<li><a href="#machine-learning-theory---univariate-linear-regression"><span class="toc-section-number">3.1</span> Machine Learning Theory - Univariate Linear Regression</a><ul>
<li><a href="#hypothesis-for-univariate-linear-regression"><span class="toc-section-number">3.1.1</span> Hypothesis for univariate linear regression</a></li>
<li><a href="#cost-function-for-univariate-linear-regression---intuition"><span class="toc-section-number">3.1.2</span> Cost function for univariate linear regression - intuition</a></li>
<li><a href="#gradient-descent-algorithm"><span class="toc-section-number">3.1.3</span> Gradient descent algorithm</a></li>
<li><a href="#gradient-descent-algorithm-for-linear-regression"><span class="toc-section-number">3.1.4</span> Gradient descent algorithm for linear regression</a></li>
</ul></li>
<li><a href="#machine-learning-theory---multivariate-linear-regression"><span class="toc-section-number">3.2</span> Machine Learning Theory - Multivariate Linear Regression</a><ul>
<li><a href="#gradient-descent-for-multiple-features"><span class="toc-section-number">3.2.1</span> Gradient descent for multiple features</a></li>
<li><a href="#changes-to-gradient-descent-feature-scaling"><span class="toc-section-number">3.2.2</span> Changes to gradient descent: feature scaling</a></li>
<li><a href="#changes-to-gradient-descent-learning-rate"><span class="toc-section-number">3.2.3</span> Changes to gradient descent: learning rate</a></li>
<li><a href="#computing-parameters-analytically-normal-equation"><span class="toc-section-number">3.2.4</span> Computing parameters analytically: normal equation</a></li>
</ul></li>
</ul></li>
<li><a href="#example-using-a-proper-modelling-approach---hyundai-elantra-car-sales"><span class="toc-section-number">4</span> Example using a proper modelling approach - Hyundai Elantra Car Sales</a><ul>
<li><a href="#goal"><span class="toc-section-number">4.1</span> Goal</a></li>
<li><a href="#dataset"><span class="toc-section-number">4.2</span> Dataset</a></li>
<li><a href="#extra-understanding"><span class="toc-section-number">4.3</span> Extra understanding</a></li>
<li><a href="#splitting-the-data"><span class="toc-section-number">4.4</span> Splitting the data</a></li>
<li><a href="#building-models"><span class="toc-section-number">4.5</span> Building models</a></li>
<li><a href="#predictions-on-train-and-test-set"><span class="toc-section-number">4.6</span> Predictions on train and test set</a></li>
<li><a href="#plotting-densities"><span class="toc-section-number">4.7</span> Plotting densities</a></li>
<li><a href="#conclusion"><span class="toc-section-number">4.8</span> Conclusion</a></li>
</ul></li>
</ul>
</div>

<style>
body {
text-align: justify}
</style>
<p><br></p>
<center>
<strong><em>This page is based on the book Machine Learning for R which I highly recommend reading.</em></strong>
</center>
<p><br></p>
<center>
website??
</center>
<p><br></p>
<p>Mathematical relationships help us to understand many aspects of everyday life. For example, body weight is a function of one’s calorie intake, income is often related to education and job experience, and poll numbers help us estimate a presidential candidate’s odds of being re-elected.</p>
<p>When such relationships are expressed with exact numbers, we gain additional clarity. For example, an additional 250 kilocalories consumed daily may result in nearly a kilogram of weight gain per month; each year of job experience may be worth an additional $1,000 in yearly salary; and a president is more likely to be re-elected when the economy is strong.</p>
<p>In this page we will work with a set of example to go from the very basic understanding of linear regression up to creating models with interactions.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ----------------------------------------------------------------------</span>
<span class="co"># Loading libraries</span>
<span class="co"># ----------------------------------------------------------------------</span>
<span class="kw">library</span>(MASS)
<span class="kw">library</span>(plotly)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(data.table)
<span class="kw">library</span>(Metrics)

<span class="co"># Other</span>
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(psych)</code></pre></div>
<p><br></p>
<div id="understanding-regression" class="section level1">
<h1><span class="header-section-number">1</span> Understanding regression</h1>
<p><br></p>
<p>Regression is concerned with specifying the relationship between a single numeric <strong>dependent variable</strong> (the value to be predicted) and one or more numeric <strong>independent variables</strong> (the predictors). As the name implies, the dependent variable depends upon the value of the independent variable or variables.</p>
<p>Let’s start by thinking about a graph with a straight line. From our high school algebra we recall that lines can be defined by a formula like <span class="math inline">\(y = a + bx\)</span>, being <span class="math inline">\(y\)</span> the <strong>dependant variable</strong>, <span class="math inline">\(a\)</span> the intercept, <span class="math inline">\(b\)</span> the coefficient that determines how much will <span class="math inline">\(y\)</span> change with a change of <span class="math inline">\(x\)</span>, and obviously <span class="math inline">\(x\)</span> being the independent variable.</p>
<p>Imagine we have the dataset shown below. In reality, we could fit infinite straight lines in the 2-D space that we have, but only 1 of them would give the best fit (we will define best fit in a later section). Given that this problem is an easy one, we could try to calculate that line by hand using various derived formulas with easy application, but we want to check how a machine and R can help us solving this problem. In other word, the machine’s job is to identify values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> so that the specified line is best able to relate the supplied <span class="math inline">\(x\)</span> values to the values of <span class="math inline">\(y\)</span>.</p>
<p><img src="images/1_linear_regression.png" width="400" /></p>
<p><br></p>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">1.1</span> Simple linear regression</h2>
<p><br></p>
<p>Linear regression is one of the most (if not the most) basic algorithms used to create predictive models. The basic idea behind linear regression is to be able to fit a straight line through the data that, at the same time, will explain or reflect as accurately as possible the real values for each point.</p>
<p>Let’s look at an example to explain how simple linear regression works. The example will be based on the <em>Boston</em> dataset that comes with the <em>MASS package</em>. Imagine that we want to understand house prices in Boston (<em>medv</em> median house val) from this dataset.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Boston)

<span class="kw">head</span>(Boston)</code></pre></div>
<pre><code>##      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat medv
## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98 24.0
## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14 21.6
## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03 34.7
## 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94 33.4
## 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33 36.2
## 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21 28.7</code></pre>
<p><br></p>
<p>Starting simple, imagine that we just want to investigate what effects does <em>lstat</em> (percent households with low socioeconomic status) have on <em>medv</em>. Plotting both variables we see that both variables are negatively correlated, which makes perfect sense. Intuitively we would say that the higher the proportion of households with low socioeconomic status, the poorer the area and hence, lower the house prices will be.</p>
<p>Out of all the lines that could cross the data (orange lines), only 1 will be the best fit (green line)</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xaxis =<span class="st"> </span><span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;lstat&quot;</span>)
yaxis =<span class="st"> </span><span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;medv&quot;</span>)

<span class="co"># Best line</span>
lm1 =<span class="st"> </span><span class="kw">lm</span>(medv ~<span class="st"> </span>lstat, <span class="dt">data =</span> Boston)
best_line =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste0</span>(<span class="st">&quot;medv ~ &quot;</span>, <span class="kw">round</span>(<span class="kw">coefficients</span>(lm1)[<span class="dv">1</span>],<span class="dv">2</span>), <span class="st">&quot; + &quot;</span>, 
                       <span class="kw">paste</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f * %s&quot;</span>, 
                                     <span class="kw">coefficients</span>(lm1)[-<span class="dv">1</span>],  
                                     <span class="kw">names</span>(<span class="kw">coefficients</span>(lm1)[-<span class="dv">1</span>])), 
                             <span class="dt">collapse=</span><span class="st">&quot; + &quot;</span>)
                        )
                      )

<span class="co"># Other line 1</span>
line1 =<span class="st"> </span>function(x){<span class="dv">40</span> -<span class="st"> </span><span class="fl">0.5</span>*x}
y1 =<span class="st"> </span><span class="kw">line1</span>(Boston$lstat)

<span class="co"># Other line 2</span>
line2 =<span class="st"> </span>function(x){<span class="dv">30</span> -<span class="st"> </span><span class="fl">0.8</span>*x}
y2 =<span class="st"> </span><span class="kw">line2</span>(Boston$lstat)


p =<span class="st"> </span>Boston %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> ~<span class="st"> </span>lstat) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">add_markers</span>(<span class="dt">y =</span> ~<span class="st"> </span>medv, <span class="dt">name =</span> <span class="st">&#39;medv vs lstat&#39;</span>) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">add_lines</span>(<span class="dt">x =</span> ~<span class="st"> </span>lstat, <span class="dt">y =</span> <span class="kw">fitted</span>(lm1), <span class="dt">name =</span> <span class="st">&#39;Best fit&#39;</span>, <span class="dt">line =</span> <span class="kw">list</span>(<span class="dt">color =</span> <span class="st">&#39;green&#39;</span>)) %&gt;%<span class="st"> </span><span class="co"># best line</span>
<span class="st">    </span><span class="kw">add_lines</span>(<span class="dt">x =</span> ~<span class="st"> </span>lstat, <span class="dt">y =</span> y1, <span class="dt">name =</span> <span class="st">&#39;Line1&#39;</span>, <span class="dt">line =</span> <span class="kw">list</span>(<span class="dt">color =</span> <span class="st">&#39;orange&#39;</span>)) %&gt;%<span class="st"> </span><span class="co"># line1</span>
<span class="st">    </span><span class="kw">add_lines</span>(<span class="dt">x =</span> ~<span class="st"> </span>lstat, <span class="dt">y =</span> y2, <span class="dt">name =</span> <span class="st">&#39;Line2&#39;</span>, <span class="dt">line =</span> <span class="kw">list</span>(<span class="dt">color =</span> <span class="st">&#39;orange&#39;</span>)) %&gt;%<span class="st"> </span><span class="co"># line2</span>
<span class="st">    </span><span class="kw">layout</span>(<span class="dt">title =</span> <span class="st">&#39;lstat vs medv&#39;</span>,
               <span class="dt">xaxis =</span> xaxis,
               <span class="dt">yaxis =</span> yaxis)

p</code></pre></div>
<div id="79c5336649" style="width:864px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="79c5336649">{"x":{"visdat":{"79c2ed96dea":["function () ","plotlyVisDat"]},"cur_data":"79c2ed96dea","attrs":{"79c2ed96dea":{"x":{},"alpha":1,"sizes":[10,100],"y":{},"type":"scatter","mode":"markers","name":"medv vs lstat"},"79c2ed96dea.1":{"x":{},"alpha":1,"sizes":[10,100],"y":[29.8225950976687,25.8703897860351,30.7251419837384,31.7606957793346,29.490077823853,29.604083746304,22.7447274121713,16.3603957549176,6.11886372140643,18.3079969301215,15.1253315950322,21.9466859550146,19.6285655318451,26.7064332173421,24.8063345098261,26.5069228530529,28.3025161316555,20.6166168597534,23.4477639339522,23.837284168993,14.5838034633901,21.4146583169101,16.7689169770335,15.6668597266742,19.0680364131278,18.8685260488387,20.4836099502273,18.136988046445,22.3932091512808,23.1722496213624,13.0827254844525,22.1651973063789,8.22797328674917,17.120435237924,15.2298370239456,25.3573631350057,23.7137777530044,26.2219080469255,24.9298409258146,30.4496276711486,32.6727431589423,29.9556020071944,29.0340541340492,27.4854736874236,25.4808695509943,24.853836977514,21.1106425237075,16.6929130287329,5.2828202900994,19.1630413485036,21.7756770713381,25.5948754734452,29.5375802915409,26.5449248272032,20.4931104437648,29.9841034878072,29.0720561081995,30.801145932039,28.0365023126033,25.7943858377344,22.0606918774655,20.8351282111177,28.1600087285918,25.5283720186822,26.9059435816313,30.1171103973333,24.8253354969013,26.8584411139434,22.117694838691,26.2029070598504,28.1695092221294,25.1673532642541,29.309568446639,27.3904687520478,28.1125062609039,26.0603996567867,23.1817501148999,24.7968340162885,22.8302318540095,25.9083917601854,29.5280797980033,27.6944845452504,28.1695092221294,27.4189702326606,25.4143660962312,28.3500185993434,22.3362061900553,26.5354243336657,29.3285694337141,29.1385595629625,26.1839060727752,26.7634361785676,26.8014381527179,28.654034392546,24.492818223086,28.2360126768925,23.7802812077675,30.554133100062,31.1621646864671,28.6730353796211,25.6043759669828,27.2669623360593,24.4548162489357,21.7851775648757,22.8397323475471,18.906528022989,16.825919938259,21.167645484933,22.8967353087726,19.7805734284463,22.2031992805292,24.9013394452019,19.1535408549661,18.317497423659,24.6258251326121,19.5810630641572,23.1152466601369,24.7683325356758,19.9515823121228,21.6236691747368,20.9016316658808,20.9966366012566,17.5194559665023,10.4130868003926,17.8519732403176,20.4836099502273,8.65549549594027,18.2224924882832,19.9325813250476,17.1299357314615,22.5832190220324,22.9062358023101,23.9892920655942,20.2745990924005,18.1084865658323,18.4410038396476,18.4980068008731,20.692620808054,14.2987886572627,17.0159298090106,11.60064849259,1.86264261657064,9.07351721159378,9.45353695309698,6.72689530781155,8.14246884491095,18.7355191393126,6.49888346290963,7.6484431809568,14.1752822412742,21.1581449913954,21.937185461477,23.0392427118363,19.5525615835444,20.1890946505623,20.2840995859381,19.2200443097291,30.1931143456339,28.4450235347192,27.5329761551115,29.3285694337141,32.9102554973818,32.7297461201678,31.3996770249066,23.4952664016401,25.2338567190172,31.0386582704785,23.0202417247611,24.0082930526694,23.7992821948426,20.8446287046553,23.1247471536745,20.5976158726782,25.9653947214109,25.3953651091561,29.490077823853,24.9488419128898,28.5780304442453,27.9794993513778,29.7655921364428,27.3714677649727,25.5758744863701,29.9746029942696,29.1575605500377,21.2721509138464,22.0606918774655,30.32612125516,28.2075111962797,30.2216158262467,29.4330748626275,29.7085891752173,30.0981094102581,31.8271992340977,29.7750926299804,30.3926247099231,31.7321942987219,30.6776395160505,26.3739159435268,28.2645141575052,30.2216158262467,30.32612125516,27.4949741809612,31.5991873891957,30.9341528415652,31.8176987405601,24.2268044040337,24.1317994686579,17.3959495505138,20.626117353291,12.6172013011111,18.1464885399826,11.7716573762665,19.3245497386425,25.6423779411331,6.47988247583447,25.5568734992949,21.7186741101126,25.3478626414682,17.5289564600399,24.5783226649242,25.328861654393,14.1657817477366,25.1198507965663,27.3334657908224,30.620636554825,30.1551123714836,31.5801864021206,28.5115269894823,30.8296474126518,30.9816553092531,23.4857659081025,29.5660817721536,32.2072189756009,30.801145932039,26.9059435816313,24.2173039104962,25.4903700445319,30.0601074361078,28.5115269894823,27.5519771421867,23.7422792336172,22.773228892784,23.8942871302184,29.6230847333791,22.6782239574082,17.0159298090106,25.8513887989599,24.9108399387395,25.509371031607,28.3215171187307,28.9485496922109,31.1431636993919,31.2001666606174,31.1906661670798,28.3120166251931,25.7658843571217,31.5991873891957,29.6895881881422,27.1529564136084,27.998500338453,25.442867576844,27.6564825711001,28.9390491986734,23.8657856496057,26.8584411139434,24.6258251326121,20.5026109373024,27.4854736874236,31.5516849215078,21.5856672005865,22.2031992805292,28.2930156381179,27.2099593748338,28.3025161316555,31.2001666606174,31.7226938051843,28.8060422891473,30.6016355677499,27.7229860258631,29.9461015136568,30.981655309253,30.1931143456339,31.6941923245715,31.5516849215078,27.0959534523829,26.7349346979548,22.2697027352923,27.770488493551,27.3334657908224,25.5188715251446,31.390176531369,31.1716651800046,30.0886089167205,26.4024174241395,24.6733276003,28.5970314313205,27.5329761551115,19.5050591158565,29.8320955912059,30.0506069425702,28.7870413020721,25.5283720186822,26.3169129823013,29.9366010201193,27.9699988578402,26.0699001503242,28.4070215605689,27.3999692455854,30.2406168133218,25.0818488224159,22.5452170478821,28.8725457439103,23.4192624533394,27.048450984695,25.7373828765089,23.6282733111662,17.1394362249991,19.4100541804807,24.7113295744503,22.4597126060439,27.7134855323256,28.0270018190657,27.2384608554466,23.4002614662643,28.7395388343842,29.7275901622925,28.7110373537715,22.4027096448184,25.0818488224159,27.5804786227994,25.917892253723,22.7447274121713,27.114954439458,29.1575605500377,28.1410077415167,26.9439455557816,25.2433572125548,24.5213197036987,26.4689208789026,25.3003601737803,25.7278823829714,29.3380699272517,26.3359139693765,27.7324865194007,30.1741133585588,24.5498211843115,22.5167155672694,28.5115269894823,28.8630452503727,28.9580501857485,28.8725457439103,29.3380699272517,27.1529564136084,30.2786187874721,26.9059435816313,29.2620659789511,17.8329722532425,21.9466859550146,23.6472742982414,22.5167155672694,27.1529564136083,21.0726405495572,24.8728379645892,20.6451183403661,29.5280797980033,27.7894894806262,21.2531499267712,21.8896829937891,31.456679986132,31.0101567898658,31.7416947922594,25.4998705380694,26.1174026180121,1.52062484921775,-1.51953308280781,21.7851775648757,12.4746938980474,14.3747926055634,12.0471716888563,13.8617659545341,18.2034915012081,14.5268005021647,12.1326761306945,11.2206287510868,5.45382917377584,5.28282029009941,7.68644515510712,4.16176205266497,5.46332966731342,14.745311853529,18.2984964365839,16.7309150028832,10.1565734748779,20.1415921828744,19.0205339454399,18.2889959430463,16.1513848970908,15.6288577525239,5.49183114792616,6.08086174725611,9.12101967928169,15.2488380110207,15.2583385045583,15.7713651555876,8.54148957348931,12.7217067300245,12.3796889626716,23.0297422182987,9.47253794017214,15.76186466205,24.9488419128898,14.3937935926385,1.90064459072096,15.4768498559226,-0.578984222587397,6.95490715271347,10.0520680459645,9.24452609527022,14.9638232048933,12.9497185749264,20.2840995859381,19.6380660253826,21.1581449913954,12.4271914303595,18.250993968896,11.3821371412257,19.6475665189202,20.7591242628171,14.1087787865111,11.6766524408907,17.7949702790921,15.8473691038883,23.1247471536745,19.1440403614285,20.1415921828744,12.4461924174347,17.4054500440514,9.42503547248424,2.23316186453625,12.8167116654003,13.5482496677939,16.0088774940271,18.792522100538,16.645410561045,11.9521667534805,11.7716573762665,17.6524628760284,18.9350295036017,17.3294460957507,16.2083878583163,17.9849801498437,17.7094658372539,18.1464885399826,18.6500146974743,16.7784174705711,17.3294460957507,16.4934026644437,18.4600048267227,19.1345398678909,20.5881153791406,18.9540304906769,20.6356178468285,21.2626504203088,24.7778330292134,21.9941884227025,21.1296435107827,18.2604944624336,14.2987886572627,17.3294460957507,20.5311124179152,19.0775369066654,22.3267056965178,20.9111321594184,23.4762654145649,17.3199456022131,11.6576514538155,16.8069189511838,10.8881114772715,17.4244510311265,22.0986938516158,24.3503108200223,27.2004588812962,27.8939949095396,24.6543266132248,21.8801825002515,24.5023187166236,20.3221015600884,23.6757757788541,17.3959495505138,11.7811578698041,6.35637605984593,17.3864490569762,21.8706820067139,23.1437481407496,21.642670161812,17.8329722532425,14.4697975409392,21.1581449913954,22.2792032288299,20.2080956376374,20.9396336400311,25.3668636285433,25.9273927472605,29.195562524188,28.3975210670313,27.0674519717701],"type":"scatter","mode":"lines","name":"Best fit","line":{"color":"green"}},"79c2ed96dea.2":{"x":{},"alpha":1,"sizes":[10,100],"y":[37.51,35.43,37.985,38.53,37.335,37.395,33.785,30.425,25.035,31.45,29.775,33.365,32.145,35.87,34.87,35.765,36.71,32.665,34.155,34.36,29.49,33.085,30.64,30.06,31.85,31.745,32.595,31.36,33.6,34.01,28.7,33.48,26.145,30.825,29.83,35.16,34.295,35.615,34.935,37.84,39.01,37.58,37.095,36.28,35.225,34.895,32.925,30.6,24.595,31.9,33.275,35.285,37.36,35.785,32.6,37.595,37.115,38.025,36.57,35.39,33.425,32.78,36.635,35.25,35.975,37.665,34.88,35.95,33.455,35.605,36.64,35.06,37.24,36.23,36.61,35.53,34.015,34.865,33.83,35.45,37.355,36.39,36.64,36.245,35.19,36.735,33.57,35.78,37.25,37.15,35.595,35.9,35.92,36.895,34.705,36.675,34.33,37.895,38.215,36.905,35.29,36.165,34.685,33.28,33.835,31.765,30.67,32.955,33.865,32.225,33.5,34.92,31.895,31.455,34.775,32.12,33.98,34.85,32.315,33.195,32.815,32.865,31.035,27.295,31.21,32.595,26.37,31.405,32.305,30.83,33.7,33.87,34.44,32.485,31.345,31.52,31.55,32.705,29.34,30.77,27.92,22.795,26.59,26.79,25.355,26.1,31.675,25.235,25.84,29.275,32.95,33.36,33.94,32.105,32.44,32.49,31.93,37.705,36.785,36.305,37.25,39.135,39.04,38.34,34.18,35.095,38.15,33.93,34.45,34.34,32.785,33.985,32.655,35.48,35.18,37.335,34.945,36.855,36.54,37.48,36.22,35.275,37.59,37.16,33.01,33.425,37.775,36.66,37.72,37.305,37.45,37.655,38.565,37.485,37.81,38.515,37.96,35.695,36.69,37.72,37.775,36.285,38.445,38.095,38.56,34.565,34.515,30.97,32.67,28.455,31.365,28.01,31.985,35.31,25.225,35.265,33.245,35.155,31.04,34.75,35.145,29.27,35.035,36.2,37.93,37.685,38.435,36.82,38.04,38.12,34.175,37.375,38.765,38.025,35.975,34.56,35.23,37.635,36.82,36.315,34.31,33.8,34.39,37.405,33.75,30.77,35.42,34.925,35.24,36.72,37.05,38.205,38.235,38.23,36.715,35.375,38.445,37.44,36.105,36.55,35.205,36.37,37.045,34.375,35.95,34.775,32.605,36.28,38.42,33.175,33.5,36.705,36.135,36.71,38.235,38.51,36.975,37.92,36.405,37.575,38.12,37.705,38.495,38.42,36.075,35.885,33.535,36.43,36.2,35.245,38.335,38.22,37.65,35.71,34.8,36.865,36.305,32.08,37.515,37.63,36.965,35.25,35.665,37.57,36.535,35.535,36.765,36.235,37.73,35.015,33.68,37.01,34.14,36.05,35.36,34.25,30.835,32.03,34.82,33.635,36.4,36.565,36.15,34.13,36.94,37.46,36.925,33.605,35.015,36.33,35.455,33.785,36.085,37.16,36.625,35.995,35.1,34.72,35.745,35.13,35.355,37.255,35.675,36.41,37.695,34.735,33.665,36.82,37.005,37.055,37.01,37.255,36.105,37.75,35.975,37.215,31.2,33.365,34.26,33.665,36.105,32.905,34.905,32.68,37.355,36.44,33,33.335,38.37,38.135,38.52,35.235,35.56,22.615,21.015,33.28,28.38,29.38,28.155,29.11,31.395,29.46,28.2,27.72,24.685,24.595,25.86,24.005,24.69,29.575,31.445,30.62,27.16,32.415,31.825,31.44,30.315,30.04,24.705,25.015,26.615,29.84,29.845,30.115,26.31,28.51,28.33,33.935,26.8,30.11,34.945,29.39,22.815,29.96,21.51,25.475,27.105,26.68,29.69,28.63,32.49,32.15,32.95,28.355,31.42,27.805,32.155,32.74,29.24,27.96,31.18,30.155,33.985,31.89,32.415,28.365,30.975,26.775,22.99,28.56,28.945,30.24,31.705,30.575,28.105,28.01,31.105,31.78,30.935,30.345,31.28,31.135,31.365,31.63,30.645,30.935,30.495,31.53,31.885,32.65,31.79,32.675,33.005,34.855,33.39,32.935,31.425,29.34,30.935,32.62,31.855,33.565,32.82,34.17,30.93,27.95,30.66,27.545,30.985,33.445,34.63,36.13,36.495,34.79,33.33,34.71,32.51,34.275,30.97,28.015,25.16,30.965,33.325,33.995,33.205,31.2,29.43,32.95,33.54,32.45,32.835,35.165,35.46,37.18,36.76,36.06],"type":"scatter","mode":"lines","name":"Line1","line":{"color":"orange"}},"79c2ed96dea.3":{"x":{},"alpha":1,"sizes":[10,100],"y":[26.016,22.688,26.776,27.648,25.736,25.832,20.056,14.68,6.056,16.32,13.64,19.384,17.432,23.392,21.792,23.224,24.736,18.264,20.648,20.976,13.184,18.936,15.024,14.096,16.96,16.792,18.152,16.176,19.76,20.416,11.92,19.568,7.832,15.32,13.728,22.256,20.872,22.984,21.896,26.544,28.416,26.128,25.352,24.048,22.36,21.832,18.68,14.96,5.352,17.04,19.24,22.456,25.776,23.256,18.16,26.152,25.384,26.84,24.512,22.624,19.48,18.448,24.616,22.4,23.56,26.264,21.808,23.52,19.528,22.968,24.624,22.096,25.584,23.968,24.576,22.848,20.424,21.784,20.128,22.72,25.768,24.224,24.624,23.992,22.304,24.776,19.712,23.248,25.6,25.44,22.952,23.44,23.472,25.032,21.528,24.68,20.928,26.632,27.144,25.048,22.464,23.864,21.496,19.248,20.136,16.824,15.072,18.728,20.184,17.56,19.6,21.872,17.032,16.328,21.64,17.392,20.368,21.76,17.704,19.112,18.504,18.584,15.656,9.672,15.936,18.152,8.192,16.248,17.688,15.328,19.92,20.192,21.104,17.976,16.152,16.432,16.48,18.328,12.944,15.232,10.672,2.472,8.544,8.864,6.568,7.76,16.68,6.376,7.344,12.84,18.72,19.376,20.304,17.368,17.904,17.984,17.088,26.328,24.856,24.088,25.6,28.616,28.464,27.344,20.688,22.152,27.04,20.288,21.12,20.944,18.456,20.376,18.248,22.768,22.288,25.736,21.912,24.968,24.464,25.968,23.952,22.44,26.144,25.456,18.816,19.48,26.44,24.656,26.352,25.688,25.92,26.248,27.704,25.976,26.496,27.624,26.736,23.112,24.704,26.352,26.44,24.056,27.512,26.952,27.696,21.304,21.224,15.552,18.272,11.528,16.184,10.816,17.176,22.496,6.36,22.424,19.192,22.248,15.664,21.6,22.232,12.832,22.056,23.92,26.688,26.296,27.496,24.912,26.864,26.992,20.68,25.8,28.024,26.84,23.56,21.296,22.368,26.216,24.912,24.104,20.896,20.08,21.024,25.848,20,15.232,22.672,21.88,22.384,24.752,25.28,27.128,27.176,27.168,24.744,22.6,27.512,25.904,23.768,24.48,22.328,24.192,25.272,21,23.52,21.64,18.168,24.048,27.472,19.08,19.6,24.728,23.816,24.736,27.176,27.616,25.16,26.672,24.248,26.12,26.992,26.328,27.592,27.472,23.72,23.416,19.656,24.288,23.92,22.392,27.336,27.152,26.24,23.136,21.68,24.984,24.088,17.328,26.024,26.208,25.144,22.4,23.064,26.112,24.456,22.856,24.824,23.976,26.368,22.024,19.888,25.216,20.624,23.68,22.576,20.8,15.336,17.248,21.712,19.816,24.24,24.504,23.84,20.608,25.104,25.936,25.08,19.768,22.024,24.128,22.728,20.056,23.736,25.456,24.6,23.592,22.16,21.552,23.192,22.208,22.568,25.608,23.08,24.256,26.312,21.576,19.864,24.912,25.208,25.288,25.216,25.608,23.768,26.4,23.56,25.544,15.92,19.384,20.816,19.864,23.768,18.648,21.848,18.288,25.768,24.304,18.8,19.336,27.392,27.016,27.632,22.376,22.896,2.184,-0.376000000000001,19.248,11.408,13.008,11.048,12.576,16.232,13.136,11.12,10.352,5.496,5.352,7.376,4.408,5.504,13.32,16.312,14.992,9.456,17.864,16.92,16.304,14.504,14.064,5.528,6.024,8.584,13.744,13.752,14.184,8.096,11.616,11.328,20.296,8.88,14.176,21.912,13.024,2.504,13.936,0.416,6.76,9.368,8.688,13.504,11.808,17.984,17.44,18.72,11.368,16.272,10.488,17.448,18.384,12.784,10.736,15.888,14.248,20.376,17.024,17.864,11.384,15.56,8.84,2.784,11.696,12.312,14.384,16.728,14.92,10.968,10.816,15.768,16.848,15.496,14.552,16.048,15.816,16.184,16.608,15.032,15.496,14.792,16.448,17.016,18.24,16.864,18.28,18.808,21.768,19.424,18.696,16.28,12.944,15.496,18.192,16.968,19.704,18.512,20.672,15.488,10.72,15.056,10.072,15.576,19.512,21.408,23.808,24.392,21.664,19.328,21.536,18.016,20.84,15.552,10.824,6.256,15.544,19.32,20.392,19.128,15.92,13.088,18.72,19.664,17.92,18.536,22.264,22.736,25.488,24.816,23.696],"type":"scatter","mode":"lines","name":"Line2","line":{"color":"orange"}}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"lstat vs medv","xaxis":{"domain":[0,1],"title":"lstat"},"yaxis":{"domain":[0,1],"title":"medv"},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[4.98,9.14,4.03,2.94,5.33,5.21,12.43,19.15,29.93,17.1,20.45,13.27,15.71,8.26,10.26,8.47,6.58,14.67,11.69,11.28,21.02,13.83,18.72,19.88,16.3,16.51,14.81,17.28,12.8,11.98,22.6,13.04,27.71,18.35,20.34,9.68,11.41,8.77,10.13,4.32,1.98,4.84,5.81,7.44,9.55,10.21,14.15,18.8,30.81,16.2,13.45,9.43,5.28,8.43,14.8,4.81,5.77,3.95,6.86,9.22,13.15,14.44,6.73,9.5,8.05,4.67,10.24,8.1,13.09,8.79,6.72,9.88,5.52,7.54,6.78,8.94,11.97,10.27,12.34,9.1,5.29,7.22,6.72,7.51,9.62,6.53,12.86,8.44,5.5,5.7,8.81,8.2,8.16,6.21,10.59,6.65,11.34,4.21,3.57,6.19,9.42,7.67,10.63,13.44,12.33,16.47,18.66,14.09,12.27,15.55,13,10.16,16.21,17.09,10.45,15.76,12.04,10.3,15.37,13.61,14.37,14.27,17.93,25.41,17.58,14.81,27.26,17.19,15.39,18.34,12.6,12.26,11.12,15.03,17.31,16.96,16.9,14.59,21.32,18.46,24.16,34.41,26.82,26.42,29.29,27.8,16.65,29.53,28.32,21.45,14.1,13.28,12.12,15.79,15.12,15.02,16.14,4.59,6.43,7.39,5.5,1.73,1.92,3.32,11.64,9.81,3.7,12.14,11.1,11.32,14.43,12.03,14.69,9.04,9.64,5.33,10.11,6.29,6.92,5.04,7.56,9.45,4.82,5.68,13.98,13.15,4.45,6.68,4.56,5.39,5.1,4.69,2.87,5.03,4.38,2.97,4.08,8.61,6.62,4.56,4.45,7.43,3.11,3.81,2.88,10.87,10.97,18.06,14.66,23.09,17.27,23.98,16.03,9.38,29.55,9.47,13.51,9.69,17.92,10.5,9.71,21.46,9.93,7.6,4.14,4.63,3.13,6.36,3.92,3.76,11.65,5.25,2.47,3.95,8.05,10.88,9.54,4.73,6.36,7.37,11.38,12.4,11.22,5.19,12.5,18.46,9.16,10.15,9.52,6.56,5.9,3.59,3.53,3.54,6.57,9.25,3.11,5.12,7.79,6.9,9.59,7.26,5.91,11.25,8.1,10.45,14.79,7.44,3.16,13.65,13,6.59,7.73,6.58,3.53,2.98,6.05,4.16,7.19,4.85,3.76,4.59,3.01,3.16,7.85,8.23,12.93,7.14,7.6,9.51,3.33,3.56,4.7,8.58,10.4,6.27,7.39,15.84,4.97,4.74,6.07,9.5,8.67,4.86,6.93,8.93,6.47,7.53,4.54,9.97,12.64,5.98,11.72,7.9,9.28,11.5,18.33,15.94,10.36,12.73,7.2,6.87,7.7,11.74,6.12,5.08,6.15,12.79,9.97,7.34,9.09,12.43,7.83,5.68,6.75,8.01,9.8,10.56,8.51,9.74,9.29,5.49,8.65,7.18,4.61,10.53,12.67,6.36,5.99,5.89,5.98,5.49,7.79,4.5,8.05,5.57,17.6,13.27,11.48,12.67,7.79,14.19,10.19,14.64,5.29,7.12,14,13.33,3.26,3.73,2.96,9.53,8.88,34.77,37.97,13.44,23.24,21.24,23.69,21.78,17.21,21.08,23.6,24.56,30.63,30.81,28.28,31.99,30.62,20.85,17.11,18.76,25.68,15.17,16.35,17.12,19.37,19.92,30.59,29.97,26.77,20.32,20.31,19.77,27.38,22.98,23.34,12.13,26.4,19.78,10.11,21.22,34.37,20.08,36.98,29.05,25.79,26.64,20.62,22.74,15.02,15.7,14.1,23.29,17.16,24.39,15.69,14.52,21.52,24.08,17.64,19.69,12.03,16.22,15.17,23.27,18.05,26.45,34.02,22.88,22.11,19.52,16.59,18.85,23.79,23.98,17.79,16.44,18.13,19.31,17.44,17.73,17.27,16.74,18.71,18.13,19.01,16.94,16.23,14.7,16.42,14.65,13.99,10.29,13.22,14.13,17.15,21.32,18.13,14.76,16.29,12.87,14.36,11.66,18.14,24.1,18.68,24.91,18.03,13.11,10.74,7.74,7.01,10.42,13.34,10.58,14.98,11.45,18.06,23.97,29.68,18.07,13.35,12.01,13.59,17.6,21.14,14.1,12.92,15.1,14.33,9.67,9.08,5.64,6.48,7.88],"y":[24,21.6,34.7,33.4,36.2,28.7,22.9,27.1,16.5,18.9,15,18.9,21.7,20.4,18.2,19.9,23.1,17.5,20.2,18.2,13.6,19.6,15.2,14.5,15.6,13.9,16.6,14.8,18.4,21,12.7,14.5,13.2,13.1,13.5,18.9,20,21,24.7,30.8,34.9,26.6,25.3,24.7,21.2,19.3,20,16.6,14.4,19.4,19.7,20.5,25,23.4,18.9,35.4,24.7,31.6,23.3,19.6,18.7,16,22.2,25,33,23.5,19.4,22,17.4,20.9,24.2,21.7,22.8,23.4,24.1,21.4,20,20.8,21.2,20.3,28,23.9,24.8,22.9,23.9,26.6,22.5,22.2,23.6,28.7,22.6,22,22.9,25,20.6,28.4,21.4,38.7,43.8,33.2,27.5,26.5,18.6,19.3,20.1,19.5,19.5,20.4,19.8,19.4,21.7,22.8,18.8,18.7,18.5,18.3,21.2,19.2,20.4,19.3,22,20.3,20.5,17.3,18.8,21.4,15.7,16.2,18,14.3,19.2,19.6,23,18.4,15.6,18.1,17.4,17.1,13.3,17.8,14,14.4,13.4,15.6,11.8,13.8,15.6,14.6,17.8,15.4,21.5,19.6,15.3,19.4,17,15.6,13.1,41.3,24.3,23.3,27,50,50,50,22.7,25,50,23.8,23.8,22.3,17.4,19.1,23.1,23.6,22.6,29.4,23.2,24.6,29.9,37.2,39.8,36.2,37.9,32.5,26.4,29.6,50,32,29.8,34.9,37,30.5,36.4,31.1,29.1,50,33.3,30.3,34.6,34.9,32.9,24.1,42.3,48.5,50,22.6,24.4,22.5,24.4,20,21.7,19.3,22.4,28.1,23.7,25,23.3,28.7,21.5,23,26.7,21.7,27.5,30.1,44.8,50,37.6,31.6,46.7,31.5,24.3,31.7,41.7,48.3,29,24,25.1,31.5,23.7,23.3,22,20.1,22.2,23.7,17.6,18.5,24.3,20.5,24.5,26.2,24.4,24.8,29.6,42.8,21.9,20.9,44,50,36,30.1,33.8,43.1,48.8,31,36.5,22.8,30.7,50,43.5,20.7,21.1,25.2,24.4,35.2,32.4,32,33.2,33.1,29.1,35.1,45.4,35.4,46,50,32.2,22,20.1,23.2,22.3,24.8,28.5,37.3,27.9,23.9,21.7,28.6,27.1,20.3,22.5,29,24.8,22,26.4,33.1,36.1,28.4,33.4,28.2,22.8,20.3,16.1,22.1,19.4,21.6,23.8,16.2,17.8,19.8,23.1,21,23.8,23.1,20.4,18.5,25,24.6,23,22.2,19.3,22.6,19.8,17.1,19.4,22.2,20.7,21.1,19.5,18.5,20.6,19,18.7,32.7,16.5,23.9,31.2,17.5,17.2,23.1,24.5,26.6,22.9,24.1,18.6,30.1,18.2,20.6,17.8,21.7,22.7,22.6,25,19.9,20.8,16.8,21.9,27.5,21.9,23.1,50,50,50,50,50,13.8,13.8,15,13.9,13.3,13.1,10.2,10.4,10.9,11.3,12.3,8.8,7.2,10.5,7.4,10.2,11.5,15.1,23.2,9.7,13.8,12.7,13.1,12.5,8.5,5,6.3,5.6,7.2,12.1,8.3,8.5,5,11.9,27.9,17.2,27.5,15,17.2,17.9,16.3,7,7.2,7.5,10.4,8.8,8.4,16.7,14.2,20.8,13.4,11.7,8.3,10.2,10.9,11,9.5,14.5,14.1,16.1,14.3,11.7,13.4,9.6,8.7,8.4,12.8,10.5,17.1,18.4,15.4,10.8,11.8,14.9,12.6,14.1,13,13.4,15.2,16.1,17.8,14.9,14.1,12.7,13.5,14.9,20,16.4,17.7,19.5,20.2,21.4,19.9,19,19.1,19.1,20.1,19.9,19.6,23.2,29.8,13.8,13.3,16.7,12,14.6,21.4,23,23.7,25,21.8,20.6,21.2,19.1,20.6,15.2,7,8.1,13.6,20.1,21.8,24.5,23.1,19.7,18.3,21.2,17.5,16.8,22.4,20.6,23.9,22,11.9],"type":"scatter","mode":"markers","name":"medv vs lstat","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(31,119,180,1)","line":{"color":"transparent"}},"xaxis":"x","yaxis":"y","frame":null},{"x":[1.73,1.92,1.98,2.47,2.87,2.88,2.94,2.96,2.97,2.98,3.01,3.11,3.11,3.13,3.16,3.16,3.26,3.32,3.33,3.53,3.53,3.54,3.56,3.57,3.59,3.7,3.73,3.76,3.76,3.81,3.92,3.95,3.95,4.03,4.08,4.14,4.16,4.21,4.32,4.38,4.45,4.45,4.5,4.54,4.56,4.56,4.59,4.59,4.61,4.63,4.67,4.69,4.7,4.73,4.74,4.81,4.82,4.84,4.85,4.86,4.97,4.98,5.03,5.04,5.08,5.1,5.12,5.19,5.21,5.25,5.28,5.29,5.29,5.33,5.33,5.39,5.49,5.49,5.5,5.5,5.52,5.57,5.64,5.68,5.68,5.7,5.77,5.81,5.89,5.9,5.91,5.98,5.98,5.99,6.05,6.07,6.12,6.15,6.19,6.21,6.27,6.29,6.36,6.36,6.36,6.43,6.47,6.48,6.53,6.56,6.57,6.58,6.58,6.59,6.62,6.65,6.68,6.72,6.72,6.73,6.75,6.78,6.86,6.87,6.9,6.92,6.93,7.01,7.12,7.14,7.18,7.19,7.2,7.22,7.26,7.34,7.37,7.39,7.39,7.43,7.44,7.44,7.51,7.53,7.54,7.56,7.6,7.6,7.67,7.7,7.73,7.74,7.79,7.79,7.79,7.83,7.85,7.88,7.9,8.01,8.05,8.05,8.05,8.1,8.1,8.16,8.2,8.23,8.26,8.43,8.44,8.47,8.51,8.58,8.61,8.65,8.67,8.77,8.79,8.81,8.88,8.93,8.94,9.04,9.08,9.09,9.1,9.14,9.16,9.22,9.25,9.28,9.29,9.38,9.42,9.43,9.45,9.47,9.5,9.5,9.51,9.52,9.53,9.54,9.55,9.59,9.62,9.64,9.67,9.68,9.69,9.71,9.74,9.8,9.81,9.88,9.93,9.97,9.97,10.11,10.11,10.13,10.15,10.16,10.19,10.21,10.24,10.26,10.27,10.29,10.3,10.36,10.4,10.42,10.45,10.45,10.5,10.53,10.56,10.58,10.59,10.63,10.74,10.87,10.88,10.97,11.1,11.12,11.22,11.25,11.28,11.32,11.34,11.38,11.41,11.45,11.48,11.5,11.64,11.65,11.66,11.69,11.72,11.74,11.97,11.98,12.01,12.03,12.03,12.04,12.12,12.13,12.14,12.26,12.27,12.33,12.34,12.4,12.43,12.43,12.5,12.6,12.64,12.67,12.67,12.73,12.79,12.8,12.86,12.87,12.92,12.93,13,13,13.04,13.09,13.11,13.15,13.15,13.22,13.27,13.27,13.28,13.33,13.34,13.35,13.44,13.44,13.45,13.51,13.59,13.61,13.65,13.83,13.98,13.99,14,14.09,14.1,14.1,14.1,14.13,14.15,14.19,14.27,14.33,14.36,14.37,14.43,14.44,14.52,14.59,14.64,14.65,14.66,14.67,14.69,14.7,14.76,14.79,14.8,14.81,14.81,14.98,15.02,15.02,15.03,15.1,15.12,15.17,15.17,15.37,15.39,15.55,15.69,15.7,15.71,15.76,15.79,15.84,15.94,16.03,16.14,16.2,16.21,16.22,16.23,16.29,16.3,16.35,16.42,16.44,16.47,16.51,16.59,16.65,16.74,16.9,16.94,16.96,17.09,17.1,17.11,17.12,17.15,17.16,17.19,17.21,17.27,17.27,17.28,17.31,17.44,17.58,17.6,17.6,17.64,17.73,17.79,17.92,17.93,18.03,18.05,18.06,18.06,18.07,18.13,18.13,18.13,18.14,18.33,18.34,18.35,18.46,18.46,18.66,18.68,18.71,18.72,18.76,18.8,18.85,19.01,19.15,19.31,19.37,19.52,19.69,19.77,19.78,19.88,19.92,20.08,20.31,20.32,20.34,20.45,20.62,20.85,21.02,21.08,21.14,21.22,21.24,21.32,21.32,21.45,21.46,21.52,21.78,22.11,22.6,22.74,22.88,22.98,23.09,23.24,23.27,23.29,23.34,23.6,23.69,23.79,23.97,23.98,23.98,24.08,24.1,24.16,24.39,24.56,24.91,25.41,25.68,25.79,26.4,26.42,26.45,26.64,26.77,26.82,27.26,27.38,27.71,27.8,28.28,28.32,29.05,29.29,29.53,29.55,29.68,29.93,29.97,30.59,30.62,30.63,30.81,30.81,31.99,34.02,34.37,34.41,34.77,36.98,37.97],"y":[32.9102554973818,32.7297461201678,32.6727431589423,32.2072189756009,31.8271992340977,31.8176987405601,31.7606957793346,31.7416947922594,31.7321942987219,31.7226938051843,31.6941923245715,31.5991873891957,31.5991873891957,31.5801864021206,31.5516849215078,31.5516849215078,31.456679986132,31.3996770249066,31.390176531369,31.2001666606174,31.2001666606174,31.1906661670798,31.1716651800046,31.1621646864671,31.1431636993919,31.0386582704785,31.0101567898658,30.9816553092531,30.981655309253,30.9341528415652,30.8296474126518,30.801145932039,30.801145932039,30.7251419837384,30.6776395160505,30.620636554825,30.6016355677499,30.554133100062,30.4496276711486,30.3926247099231,30.32612125516,30.32612125516,30.2786187874721,30.2406168133218,30.2216158262467,30.2216158262467,30.1931143456339,30.1931143456339,30.1741133585588,30.1551123714836,30.1171103973333,30.0981094102581,30.0886089167205,30.0601074361078,30.0506069425702,29.9841034878072,29.9746029942696,29.9556020071944,29.9461015136568,29.9366010201193,29.8320955912059,29.8225950976687,29.7750926299804,29.7655921364428,29.7275901622925,29.7085891752173,29.6895881881422,29.6230847333791,29.604083746304,29.5660817721536,29.5375802915409,29.5280797980033,29.5280797980033,29.490077823853,29.490077823853,29.4330748626275,29.3380699272517,29.3380699272517,29.3285694337141,29.3285694337141,29.309568446639,29.2620659789511,29.195562524188,29.1575605500377,29.1575605500377,29.1385595629625,29.0720561081995,29.0340541340492,28.9580501857485,28.9485496922109,28.9390491986734,28.8725457439103,28.8725457439103,28.8630452503727,28.8060422891473,28.7870413020721,28.7395388343842,28.7110373537715,28.6730353796211,28.654034392546,28.5970314313205,28.5780304442453,28.5115269894823,28.5115269894823,28.5115269894823,28.4450235347192,28.4070215605689,28.3975210670313,28.3500185993434,28.3215171187307,28.3120166251931,28.3025161316555,28.3025161316555,28.2930156381179,28.2645141575052,28.2360126768925,28.2075111962797,28.1695092221294,28.1695092221294,28.1600087285918,28.1410077415167,28.1125062609039,28.0365023126033,28.0270018190657,27.998500338453,27.9794993513778,27.9699988578402,27.8939949095396,27.7894894806262,27.770488493551,27.7324865194007,27.7229860258631,27.7134855323256,27.6944845452504,27.6564825711001,27.5804786227994,27.5519771421867,27.5329761551115,27.5329761551115,27.4949741809612,27.4854736874236,27.4854736874236,27.4189702326606,27.3999692455854,27.3904687520478,27.3714677649727,27.3334657908224,27.3334657908224,27.2669623360593,27.2384608554466,27.2099593748338,27.2004588812962,27.1529564136084,27.1529564136084,27.1529564136083,27.114954439458,27.0959534523829,27.0674519717701,27.048450984695,26.9439455557816,26.9059435816313,26.9059435816313,26.9059435816313,26.8584411139434,26.8584411139434,26.8014381527179,26.7634361785676,26.7349346979548,26.7064332173421,26.5449248272032,26.5354243336657,26.5069228530529,26.4689208789026,26.4024174241395,26.3739159435268,26.3359139693765,26.3169129823013,26.2219080469255,26.2029070598504,26.1839060727752,26.1174026180121,26.0699001503242,26.0603996567867,25.9653947214109,25.9273927472605,25.917892253723,25.9083917601854,25.8703897860351,25.8513887989599,25.7943858377344,25.7658843571217,25.7373828765089,25.7278823829714,25.6423779411331,25.6043759669828,25.5948754734452,25.5758744863701,25.5568734992949,25.5283720186822,25.5283720186822,25.5188715251446,25.509371031607,25.4998705380694,25.4903700445319,25.4808695509943,25.442867576844,25.4143660962312,25.3953651091561,25.3668636285433,25.3573631350057,25.3478626414682,25.328861654393,25.3003601737803,25.2433572125548,25.2338567190172,25.1673532642541,25.1198507965663,25.0818488224159,25.0818488224159,24.9488419128898,24.9488419128898,24.9298409258146,24.9108399387395,24.9013394452019,24.8728379645892,24.853836977514,24.8253354969013,24.8063345098261,24.7968340162885,24.7778330292134,24.7683325356758,24.7113295744503,24.6733276003,24.6543266132248,24.6258251326121,24.6258251326121,24.5783226649242,24.5498211843115,24.5213197036987,24.5023187166236,24.492818223086,24.4548162489357,24.3503108200223,24.2268044040337,24.2173039104962,24.1317994686579,24.0082930526694,23.9892920655942,23.8942871302184,23.8657856496057,23.837284168993,23.7992821948426,23.7802812077675,23.7422792336172,23.7137777530044,23.6757757788541,23.6472742982414,23.6282733111662,23.4952664016401,23.4857659081025,23.4762654145649,23.4477639339522,23.4192624533394,23.4002614662643,23.1817501148999,23.1722496213624,23.1437481407496,23.1247471536745,23.1247471536745,23.1152466601369,23.0392427118363,23.0297422182987,23.0202417247611,22.9062358023101,22.8967353087726,22.8397323475471,22.8302318540095,22.773228892784,22.7447274121713,22.7447274121713,22.6782239574082,22.5832190220324,22.5452170478821,22.5167155672694,22.5167155672694,22.4597126060439,22.4027096448184,22.3932091512808,22.3362061900553,22.3267056965178,22.2792032288299,22.2697027352923,22.2031992805292,22.2031992805292,22.1651973063789,22.117694838691,22.0986938516158,22.0606918774655,22.0606918774655,21.9941884227025,21.9466859550146,21.9466859550146,21.937185461477,21.8896829937891,21.8801825002515,21.8706820067139,21.7851775648757,21.7851775648757,21.7756770713381,21.7186741101126,21.642670161812,21.6236691747368,21.5856672005865,21.4146583169101,21.2721509138464,21.2626504203088,21.2531499267712,21.167645484933,21.1581449913954,21.1581449913954,21.1581449913954,21.1296435107827,21.1106425237075,21.0726405495572,20.9966366012566,20.9396336400311,20.9111321594184,20.9016316658808,20.8446287046553,20.8351282111177,20.7591242628171,20.692620808054,20.6451183403661,20.6356178468285,20.626117353291,20.6166168597534,20.5976158726782,20.5881153791406,20.5311124179152,20.5026109373024,20.4931104437648,20.4836099502273,20.4836099502273,20.3221015600884,20.2840995859381,20.2840995859381,20.2745990924005,20.2080956376374,20.1890946505623,20.1415921828744,20.1415921828744,19.9515823121228,19.9325813250476,19.7805734284463,19.6475665189202,19.6380660253826,19.6285655318451,19.5810630641572,19.5525615835444,19.5050591158565,19.4100541804807,19.3245497386425,19.2200443097291,19.1630413485036,19.1535408549661,19.1440403614285,19.1345398678909,19.0775369066654,19.0680364131278,19.0205339454399,18.9540304906769,18.9350295036017,18.906528022989,18.8685260488387,18.792522100538,18.7355191393126,18.6500146974743,18.4980068008731,18.4600048267227,18.4410038396476,18.317497423659,18.3079969301215,18.2984964365839,18.2889959430463,18.2604944624336,18.250993968896,18.2224924882832,18.2034915012081,18.1464885399826,18.1464885399826,18.136988046445,18.1084865658323,17.9849801498437,17.8519732403176,17.8329722532425,17.8329722532425,17.7949702790921,17.7094658372539,17.6524628760284,17.5289564600399,17.5194559665023,17.4244510311265,17.4054500440514,17.3959495505138,17.3959495505138,17.3864490569762,17.3294460957507,17.3294460957507,17.3294460957507,17.3199456022131,17.1394362249991,17.1299357314615,17.120435237924,17.0159298090106,17.0159298090106,16.825919938259,16.8069189511838,16.7784174705711,16.7689169770335,16.7309150028832,16.6929130287329,16.645410561045,16.4934026644437,16.3603957549176,16.2083878583163,16.1513848970908,16.0088774940271,15.8473691038883,15.7713651555876,15.76186466205,15.6668597266742,15.6288577525239,15.4768498559226,15.2583385045583,15.2488380110207,15.2298370239456,15.1253315950322,14.9638232048933,14.745311853529,14.5838034633901,14.5268005021647,14.4697975409392,14.3937935926385,14.3747926055634,14.2987886572627,14.2987886572627,14.1752822412742,14.1657817477366,14.1087787865111,13.8617659545341,13.5482496677939,13.0827254844525,12.9497185749264,12.8167116654003,12.7217067300245,12.6172013011111,12.4746938980474,12.4461924174347,12.4271914303595,12.3796889626716,12.1326761306945,12.0471716888563,11.9521667534805,11.7811578698041,11.7716573762665,11.7716573762665,11.6766524408907,11.6576514538155,11.60064849259,11.3821371412257,11.2206287510868,10.8881114772715,10.4130868003926,10.1565734748779,10.0520680459645,9.47253794017214,9.45353695309698,9.42503547248424,9.24452609527022,9.12101967928169,9.07351721159378,8.65549549594027,8.54148957348931,8.22797328674917,8.14246884491095,7.68644515510712,7.6484431809568,6.95490715271347,6.72689530781155,6.49888346290963,6.47988247583447,6.35637605984593,6.11886372140643,6.08086174725611,5.49183114792616,5.46332966731342,5.45382917377584,5.2828202900994,5.28282029009941,4.16176205266497,2.23316186453625,1.90064459072096,1.86264261657064,1.52062484921775,-0.578984222587397,-1.51953308280781],"type":"scatter","mode":"lines","name":"Best fit","line":{"fillcolor":"rgba(255,127,14,1)","color":"green"},"xaxis":"x","yaxis":"y","frame":null},{"x":[1.73,1.92,1.98,2.47,2.87,2.88,2.94,2.96,2.97,2.98,3.01,3.11,3.11,3.13,3.16,3.16,3.26,3.32,3.33,3.53,3.53,3.54,3.56,3.57,3.59,3.7,3.73,3.76,3.76,3.81,3.92,3.95,3.95,4.03,4.08,4.14,4.16,4.21,4.32,4.38,4.45,4.45,4.5,4.54,4.56,4.56,4.59,4.59,4.61,4.63,4.67,4.69,4.7,4.73,4.74,4.81,4.82,4.84,4.85,4.86,4.97,4.98,5.03,5.04,5.08,5.1,5.12,5.19,5.21,5.25,5.28,5.29,5.29,5.33,5.33,5.39,5.49,5.49,5.5,5.5,5.52,5.57,5.64,5.68,5.68,5.7,5.77,5.81,5.89,5.9,5.91,5.98,5.98,5.99,6.05,6.07,6.12,6.15,6.19,6.21,6.27,6.29,6.36,6.36,6.36,6.43,6.47,6.48,6.53,6.56,6.57,6.58,6.58,6.59,6.62,6.65,6.68,6.72,6.72,6.73,6.75,6.78,6.86,6.87,6.9,6.92,6.93,7.01,7.12,7.14,7.18,7.19,7.2,7.22,7.26,7.34,7.37,7.39,7.39,7.43,7.44,7.44,7.51,7.53,7.54,7.56,7.6,7.6,7.67,7.7,7.73,7.74,7.79,7.79,7.79,7.83,7.85,7.88,7.9,8.01,8.05,8.05,8.05,8.1,8.1,8.16,8.2,8.23,8.26,8.43,8.44,8.47,8.51,8.58,8.61,8.65,8.67,8.77,8.79,8.81,8.88,8.93,8.94,9.04,9.08,9.09,9.1,9.14,9.16,9.22,9.25,9.28,9.29,9.38,9.42,9.43,9.45,9.47,9.5,9.5,9.51,9.52,9.53,9.54,9.55,9.59,9.62,9.64,9.67,9.68,9.69,9.71,9.74,9.8,9.81,9.88,9.93,9.97,9.97,10.11,10.11,10.13,10.15,10.16,10.19,10.21,10.24,10.26,10.27,10.29,10.3,10.36,10.4,10.42,10.45,10.45,10.5,10.53,10.56,10.58,10.59,10.63,10.74,10.87,10.88,10.97,11.1,11.12,11.22,11.25,11.28,11.32,11.34,11.38,11.41,11.45,11.48,11.5,11.64,11.65,11.66,11.69,11.72,11.74,11.97,11.98,12.01,12.03,12.03,12.04,12.12,12.13,12.14,12.26,12.27,12.33,12.34,12.4,12.43,12.43,12.5,12.6,12.64,12.67,12.67,12.73,12.79,12.8,12.86,12.87,12.92,12.93,13,13,13.04,13.09,13.11,13.15,13.15,13.22,13.27,13.27,13.28,13.33,13.34,13.35,13.44,13.44,13.45,13.51,13.59,13.61,13.65,13.83,13.98,13.99,14,14.09,14.1,14.1,14.1,14.13,14.15,14.19,14.27,14.33,14.36,14.37,14.43,14.44,14.52,14.59,14.64,14.65,14.66,14.67,14.69,14.7,14.76,14.79,14.8,14.81,14.81,14.98,15.02,15.02,15.03,15.1,15.12,15.17,15.17,15.37,15.39,15.55,15.69,15.7,15.71,15.76,15.79,15.84,15.94,16.03,16.14,16.2,16.21,16.22,16.23,16.29,16.3,16.35,16.42,16.44,16.47,16.51,16.59,16.65,16.74,16.9,16.94,16.96,17.09,17.1,17.11,17.12,17.15,17.16,17.19,17.21,17.27,17.27,17.28,17.31,17.44,17.58,17.6,17.6,17.64,17.73,17.79,17.92,17.93,18.03,18.05,18.06,18.06,18.07,18.13,18.13,18.13,18.14,18.33,18.34,18.35,18.46,18.46,18.66,18.68,18.71,18.72,18.76,18.8,18.85,19.01,19.15,19.31,19.37,19.52,19.69,19.77,19.78,19.88,19.92,20.08,20.31,20.32,20.34,20.45,20.62,20.85,21.02,21.08,21.14,21.22,21.24,21.32,21.32,21.45,21.46,21.52,21.78,22.11,22.6,22.74,22.88,22.98,23.09,23.24,23.27,23.29,23.34,23.6,23.69,23.79,23.97,23.98,23.98,24.08,24.1,24.16,24.39,24.56,24.91,25.41,25.68,25.79,26.4,26.42,26.45,26.64,26.77,26.82,27.26,27.38,27.71,27.8,28.28,28.32,29.05,29.29,29.53,29.55,29.68,29.93,29.97,30.59,30.62,30.63,30.81,30.81,31.99,34.02,34.37,34.41,34.77,36.98,37.97],"y":[39.135,39.04,39.01,38.765,38.565,38.56,38.53,38.52,38.515,38.51,38.495,38.445,38.445,38.435,38.42,38.42,38.37,38.34,38.335,38.235,38.235,38.23,38.22,38.215,38.205,38.15,38.135,38.12,38.12,38.095,38.04,38.025,38.025,37.985,37.96,37.93,37.92,37.895,37.84,37.81,37.775,37.775,37.75,37.73,37.72,37.72,37.705,37.705,37.695,37.685,37.665,37.655,37.65,37.635,37.63,37.595,37.59,37.58,37.575,37.57,37.515,37.51,37.485,37.48,37.46,37.45,37.44,37.405,37.395,37.375,37.36,37.355,37.355,37.335,37.335,37.305,37.255,37.255,37.25,37.25,37.24,37.215,37.18,37.16,37.16,37.15,37.115,37.095,37.055,37.05,37.045,37.01,37.01,37.005,36.975,36.965,36.94,36.925,36.905,36.895,36.865,36.855,36.82,36.82,36.82,36.785,36.765,36.76,36.735,36.72,36.715,36.71,36.71,36.705,36.69,36.675,36.66,36.64,36.64,36.635,36.625,36.61,36.57,36.565,36.55,36.54,36.535,36.495,36.44,36.43,36.41,36.405,36.4,36.39,36.37,36.33,36.315,36.305,36.305,36.285,36.28,36.28,36.245,36.235,36.23,36.22,36.2,36.2,36.165,36.15,36.135,36.13,36.105,36.105,36.105,36.085,36.075,36.06,36.05,35.995,35.975,35.975,35.975,35.95,35.95,35.92,35.9,35.885,35.87,35.785,35.78,35.765,35.745,35.71,35.695,35.675,35.665,35.615,35.605,35.595,35.56,35.535,35.53,35.48,35.46,35.455,35.45,35.43,35.42,35.39,35.375,35.36,35.355,35.31,35.29,35.285,35.275,35.265,35.25,35.25,35.245,35.24,35.235,35.23,35.225,35.205,35.19,35.18,35.165,35.16,35.155,35.145,35.13,35.1,35.095,35.06,35.035,35.015,35.015,34.945,34.945,34.935,34.925,34.92,34.905,34.895,34.88,34.87,34.865,34.855,34.85,34.82,34.8,34.79,34.775,34.775,34.75,34.735,34.72,34.71,34.705,34.685,34.63,34.565,34.56,34.515,34.45,34.44,34.39,34.375,34.36,34.34,34.33,34.31,34.295,34.275,34.26,34.25,34.18,34.175,34.17,34.155,34.14,34.13,34.015,34.01,33.995,33.985,33.985,33.98,33.94,33.935,33.93,33.87,33.865,33.835,33.83,33.8,33.785,33.785,33.75,33.7,33.68,33.665,33.665,33.635,33.605,33.6,33.57,33.565,33.54,33.535,33.5,33.5,33.48,33.455,33.445,33.425,33.425,33.39,33.365,33.365,33.36,33.335,33.33,33.325,33.28,33.28,33.275,33.245,33.205,33.195,33.175,33.085,33.01,33.005,33,32.955,32.95,32.95,32.95,32.935,32.925,32.905,32.865,32.835,32.82,32.815,32.785,32.78,32.74,32.705,32.68,32.675,32.67,32.665,32.655,32.65,32.62,32.605,32.6,32.595,32.595,32.51,32.49,32.49,32.485,32.45,32.44,32.415,32.415,32.315,32.305,32.225,32.155,32.15,32.145,32.12,32.105,32.08,32.03,31.985,31.93,31.9,31.895,31.89,31.885,31.855,31.85,31.825,31.79,31.78,31.765,31.745,31.705,31.675,31.63,31.55,31.53,31.52,31.455,31.45,31.445,31.44,31.425,31.42,31.405,31.395,31.365,31.365,31.36,31.345,31.28,31.21,31.2,31.2,31.18,31.135,31.105,31.04,31.035,30.985,30.975,30.97,30.97,30.965,30.935,30.935,30.935,30.93,30.835,30.83,30.825,30.77,30.77,30.67,30.66,30.645,30.64,30.62,30.6,30.575,30.495,30.425,30.345,30.315,30.24,30.155,30.115,30.11,30.06,30.04,29.96,29.845,29.84,29.83,29.775,29.69,29.575,29.49,29.46,29.43,29.39,29.38,29.34,29.34,29.275,29.27,29.24,29.11,28.945,28.7,28.63,28.56,28.51,28.455,28.38,28.365,28.355,28.33,28.2,28.155,28.105,28.015,28.01,28.01,27.96,27.95,27.92,27.805,27.72,27.545,27.295,27.16,27.105,26.8,26.79,26.775,26.68,26.615,26.59,26.37,26.31,26.145,26.1,25.86,25.84,25.475,25.355,25.235,25.225,25.16,25.035,25.015,24.705,24.69,24.685,24.595,24.595,24.005,22.99,22.815,22.795,22.615,21.51,21.015],"type":"scatter","mode":"lines","name":"Line1","line":{"fillcolor":"rgba(44,160,44,1)","color":"orange"},"xaxis":"x","yaxis":"y","frame":null},{"x":[1.73,1.92,1.98,2.47,2.87,2.88,2.94,2.96,2.97,2.98,3.01,3.11,3.11,3.13,3.16,3.16,3.26,3.32,3.33,3.53,3.53,3.54,3.56,3.57,3.59,3.7,3.73,3.76,3.76,3.81,3.92,3.95,3.95,4.03,4.08,4.14,4.16,4.21,4.32,4.38,4.45,4.45,4.5,4.54,4.56,4.56,4.59,4.59,4.61,4.63,4.67,4.69,4.7,4.73,4.74,4.81,4.82,4.84,4.85,4.86,4.97,4.98,5.03,5.04,5.08,5.1,5.12,5.19,5.21,5.25,5.28,5.29,5.29,5.33,5.33,5.39,5.49,5.49,5.5,5.5,5.52,5.57,5.64,5.68,5.68,5.7,5.77,5.81,5.89,5.9,5.91,5.98,5.98,5.99,6.05,6.07,6.12,6.15,6.19,6.21,6.27,6.29,6.36,6.36,6.36,6.43,6.47,6.48,6.53,6.56,6.57,6.58,6.58,6.59,6.62,6.65,6.68,6.72,6.72,6.73,6.75,6.78,6.86,6.87,6.9,6.92,6.93,7.01,7.12,7.14,7.18,7.19,7.2,7.22,7.26,7.34,7.37,7.39,7.39,7.43,7.44,7.44,7.51,7.53,7.54,7.56,7.6,7.6,7.67,7.7,7.73,7.74,7.79,7.79,7.79,7.83,7.85,7.88,7.9,8.01,8.05,8.05,8.05,8.1,8.1,8.16,8.2,8.23,8.26,8.43,8.44,8.47,8.51,8.58,8.61,8.65,8.67,8.77,8.79,8.81,8.88,8.93,8.94,9.04,9.08,9.09,9.1,9.14,9.16,9.22,9.25,9.28,9.29,9.38,9.42,9.43,9.45,9.47,9.5,9.5,9.51,9.52,9.53,9.54,9.55,9.59,9.62,9.64,9.67,9.68,9.69,9.71,9.74,9.8,9.81,9.88,9.93,9.97,9.97,10.11,10.11,10.13,10.15,10.16,10.19,10.21,10.24,10.26,10.27,10.29,10.3,10.36,10.4,10.42,10.45,10.45,10.5,10.53,10.56,10.58,10.59,10.63,10.74,10.87,10.88,10.97,11.1,11.12,11.22,11.25,11.28,11.32,11.34,11.38,11.41,11.45,11.48,11.5,11.64,11.65,11.66,11.69,11.72,11.74,11.97,11.98,12.01,12.03,12.03,12.04,12.12,12.13,12.14,12.26,12.27,12.33,12.34,12.4,12.43,12.43,12.5,12.6,12.64,12.67,12.67,12.73,12.79,12.8,12.86,12.87,12.92,12.93,13,13,13.04,13.09,13.11,13.15,13.15,13.22,13.27,13.27,13.28,13.33,13.34,13.35,13.44,13.44,13.45,13.51,13.59,13.61,13.65,13.83,13.98,13.99,14,14.09,14.1,14.1,14.1,14.13,14.15,14.19,14.27,14.33,14.36,14.37,14.43,14.44,14.52,14.59,14.64,14.65,14.66,14.67,14.69,14.7,14.76,14.79,14.8,14.81,14.81,14.98,15.02,15.02,15.03,15.1,15.12,15.17,15.17,15.37,15.39,15.55,15.69,15.7,15.71,15.76,15.79,15.84,15.94,16.03,16.14,16.2,16.21,16.22,16.23,16.29,16.3,16.35,16.42,16.44,16.47,16.51,16.59,16.65,16.74,16.9,16.94,16.96,17.09,17.1,17.11,17.12,17.15,17.16,17.19,17.21,17.27,17.27,17.28,17.31,17.44,17.58,17.6,17.6,17.64,17.73,17.79,17.92,17.93,18.03,18.05,18.06,18.06,18.07,18.13,18.13,18.13,18.14,18.33,18.34,18.35,18.46,18.46,18.66,18.68,18.71,18.72,18.76,18.8,18.85,19.01,19.15,19.31,19.37,19.52,19.69,19.77,19.78,19.88,19.92,20.08,20.31,20.32,20.34,20.45,20.62,20.85,21.02,21.08,21.14,21.22,21.24,21.32,21.32,21.45,21.46,21.52,21.78,22.11,22.6,22.74,22.88,22.98,23.09,23.24,23.27,23.29,23.34,23.6,23.69,23.79,23.97,23.98,23.98,24.08,24.1,24.16,24.39,24.56,24.91,25.41,25.68,25.79,26.4,26.42,26.45,26.64,26.77,26.82,27.26,27.38,27.71,27.8,28.28,28.32,29.05,29.29,29.53,29.55,29.68,29.93,29.97,30.59,30.62,30.63,30.81,30.81,31.99,34.02,34.37,34.41,34.77,36.98,37.97],"y":[28.616,28.464,28.416,28.024,27.704,27.696,27.648,27.632,27.624,27.616,27.592,27.512,27.512,27.496,27.472,27.472,27.392,27.344,27.336,27.176,27.176,27.168,27.152,27.144,27.128,27.04,27.016,26.992,26.992,26.952,26.864,26.84,26.84,26.776,26.736,26.688,26.672,26.632,26.544,26.496,26.44,26.44,26.4,26.368,26.352,26.352,26.328,26.328,26.312,26.296,26.264,26.248,26.24,26.216,26.208,26.152,26.144,26.128,26.12,26.112,26.024,26.016,25.976,25.968,25.936,25.92,25.904,25.848,25.832,25.8,25.776,25.768,25.768,25.736,25.736,25.688,25.608,25.608,25.6,25.6,25.584,25.544,25.488,25.456,25.456,25.44,25.384,25.352,25.288,25.28,25.272,25.216,25.216,25.208,25.16,25.144,25.104,25.08,25.048,25.032,24.984,24.968,24.912,24.912,24.912,24.856,24.824,24.816,24.776,24.752,24.744,24.736,24.736,24.728,24.704,24.68,24.656,24.624,24.624,24.616,24.6,24.576,24.512,24.504,24.48,24.464,24.456,24.392,24.304,24.288,24.256,24.248,24.24,24.224,24.192,24.128,24.104,24.088,24.088,24.056,24.048,24.048,23.992,23.976,23.968,23.952,23.92,23.92,23.864,23.84,23.816,23.808,23.768,23.768,23.768,23.736,23.72,23.696,23.68,23.592,23.56,23.56,23.56,23.52,23.52,23.472,23.44,23.416,23.392,23.256,23.248,23.224,23.192,23.136,23.112,23.08,23.064,22.984,22.968,22.952,22.896,22.856,22.848,22.768,22.736,22.728,22.72,22.688,22.672,22.624,22.6,22.576,22.568,22.496,22.464,22.456,22.44,22.424,22.4,22.4,22.392,22.384,22.376,22.368,22.36,22.328,22.304,22.288,22.264,22.256,22.248,22.232,22.208,22.16,22.152,22.096,22.056,22.024,22.024,21.912,21.912,21.896,21.88,21.872,21.848,21.832,21.808,21.792,21.784,21.768,21.76,21.712,21.68,21.664,21.64,21.64,21.6,21.576,21.552,21.536,21.528,21.496,21.408,21.304,21.296,21.224,21.12,21.104,21.024,21,20.976,20.944,20.928,20.896,20.872,20.84,20.816,20.8,20.688,20.68,20.672,20.648,20.624,20.608,20.424,20.416,20.392,20.376,20.376,20.368,20.304,20.296,20.288,20.192,20.184,20.136,20.128,20.08,20.056,20.056,20,19.92,19.888,19.864,19.864,19.816,19.768,19.76,19.712,19.704,19.664,19.656,19.6,19.6,19.568,19.528,19.512,19.48,19.48,19.424,19.384,19.384,19.376,19.336,19.328,19.32,19.248,19.248,19.24,19.192,19.128,19.112,19.08,18.936,18.816,18.808,18.8,18.728,18.72,18.72,18.72,18.696,18.68,18.648,18.584,18.536,18.512,18.504,18.456,18.448,18.384,18.328,18.288,18.28,18.272,18.264,18.248,18.24,18.192,18.168,18.16,18.152,18.152,18.016,17.984,17.984,17.976,17.92,17.904,17.864,17.864,17.704,17.688,17.56,17.448,17.44,17.432,17.392,17.368,17.328,17.248,17.176,17.088,17.04,17.032,17.024,17.016,16.968,16.96,16.92,16.864,16.848,16.824,16.792,16.728,16.68,16.608,16.48,16.448,16.432,16.328,16.32,16.312,16.304,16.28,16.272,16.248,16.232,16.184,16.184,16.176,16.152,16.048,15.936,15.92,15.92,15.888,15.816,15.768,15.664,15.656,15.576,15.56,15.552,15.552,15.544,15.496,15.496,15.496,15.488,15.336,15.328,15.32,15.232,15.232,15.072,15.056,15.032,15.024,14.992,14.96,14.92,14.792,14.68,14.552,14.504,14.384,14.248,14.184,14.176,14.096,14.064,13.936,13.752,13.744,13.728,13.64,13.504,13.32,13.184,13.136,13.088,13.024,13.008,12.944,12.944,12.84,12.832,12.784,12.576,12.312,11.92,11.808,11.696,11.616,11.528,11.408,11.384,11.368,11.328,11.12,11.048,10.968,10.824,10.816,10.816,10.736,10.72,10.672,10.488,10.352,10.072,9.672,9.456,9.368,8.88,8.864,8.84,8.688,8.584,8.544,8.192,8.096,7.832,7.76,7.376,7.344,6.76,6.568,6.376,6.36,6.256,6.056,6.024,5.528,5.504,5.496,5.352,5.352,4.408,2.784,2.504,2.472,2.184,0.416,-0.376000000000001],"type":"scatter","mode":"lines","name":"Line2","line":{"fillcolor":"rgba(214,39,40,1)","color":"orange"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<p><br></p>
<div id="line-of-best-fit" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Line of best fit</h3>
<p><br></p>
<p>So, the question is, how does the machine know (or how would we know if we calculated it) that the green line is the line of best fit?</p>
<p>In order to determine the optimal estimates of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, an estimation method known as Ordinary Least Squares (OLS) was used. In OLS regression, the slope and intercept are chosen so that they minimize the sum of the squared errors, that is, the vertical distance between the predicted y value and the actual y value. These errors are known as residuals. In mathematical terms, the goal is to minimize the sum of squared errors, in other words <span class="math inline">\(min \sum {e_i}^2 = \sum {(y_i - \widehat{y_i})}^2\)</span>.</p>
<p>To understand this more clearly, check the following image.</p>
<p><br></p>
<p><br></p>
<p>Let’s check the errors made by the 3 lines plotted above, and check if the green line is really the best out of the 3 based on the OLS method and the residuals.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Best line</span>
lm1 =<span class="st"> </span><span class="kw">lm</span>(medv ~<span class="st"> </span>lstat, <span class="dt">data =</span> Boston)

<span class="co"># Other line 1</span>
line1 =<span class="st"> </span>function(x){<span class="dv">40</span> -<span class="st"> </span><span class="fl">0.5</span>*x}
y1 =<span class="st"> </span><span class="kw">line1</span>(Boston$lstat)

<span class="co"># Other line 2</span>
line2 =<span class="st"> </span>function(x){<span class="dv">30</span> -<span class="st"> </span><span class="fl">0.8</span>*x}
y2 =<span class="st"> </span><span class="kw">line2</span>(Boston$lstat)

<span class="co"># Creating dataframe to show errors</span>
df =<span class="st"> </span>Boston[, <span class="kw">c</span>(<span class="st">&#39;lstat&#39;</span>,<span class="st">&#39;medv&#39;</span>)]
df$medv_best_line =<span class="st"> </span><span class="kw">round</span>(<span class="kw">predict</span>(lm1, df),<span class="dv">1</span>)
df$medv_line1 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">line1</span>(df$lstat),<span class="dv">1</span>)
df$medv_line2 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">line2</span>(df$lstat),<span class="dv">1</span>)
df$error_best_line =<span class="st"> </span>(df$medv -<span class="st"> </span>df$medv_best_line)^<span class="dv">2</span>
df$error_line1 =<span class="st"> </span>(df$medv -<span class="st"> </span>df$medv_line1)^<span class="dv">2</span>
df$error_line2 =<span class="st"> </span>(df$medv -<span class="st"> </span>df$medv_line2)^<span class="dv">2</span>

<span class="co"># Show</span>
<span class="kw">head</span>(df)</code></pre></div>
<pre><code>##   lstat medv medv_best_line medv_line1 medv_line2 error_best_line error_line1 error_line2
## 1  4.98 24.0           29.8       37.5       26.0           33.64      182.25        4.00
## 2  9.14 21.6           25.9       35.4       22.7           18.49      190.44        1.21
## 3  4.03 34.7           30.7       38.0       26.8           16.00       10.89       62.41
## 4  2.94 33.4           31.8       38.5       27.6            2.56       26.01       33.64
## 5  5.33 36.2           29.5       37.3       25.7           44.89        1.21      110.25
## 6  5.21 28.7           29.6       37.4       25.8            0.81       75.69        8.41</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print sum of errors</span>
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Sum of errors best line&#39;</span>,<span class="kw">sum</span>(df$error_best_line)))</code></pre></div>
<pre><code>## [1] &quot;Sum of errors best line 19457.41&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&#39;&#39;</span>)</code></pre></div>
<pre><code>## [1] &quot;&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Sum of errors line 1&#39;</span>,<span class="kw">sum</span>(df$error_line1)))</code></pre></div>
<pre><code>## [1] &quot;Sum of errors line 1 87496.23&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&#39;&#39;</span>)</code></pre></div>
<pre><code>## [1] &quot;&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Sum of errors line 2&#39;</span>,<span class="kw">sum</span>(df$error_line2)))</code></pre></div>
<pre><code>## [1] &quot;Sum of errors line 2 23618.84&quot;</code></pre>
<p><br></p>
</div>
<div id="correlations" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Correlations</h3>
<p><br></p>
<p>We now know that, for a linear regression, there is only 1 possible line of best fit which is built minimizing the errors made using the Ordinary Least Squares method. However, having this line of best fit does not inform us of how strong is the relationship between 2 variables.</p>
<p>To understand this we use correlation, which indicates how closely their relationship follows a straight line. Correlation typically refers to the <em>Pearson coefficient</em>, which ranges from -1 to +1, being -1 a perfectly negative relationship, +1 perfectly positive relationship and 0 being no relationship. For mathematical completion, the formula de calculate the Pearson coefficient is:</p>
<p><span class="math display">\[ \rho_{x,y} = Corr(x,y) = \frac{Cov(x,y)}{\sigma_{x}\sigma_{y}} \]</span> Let’s understand the correlation values with a couple of examples:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># You can print all possible correlations of all variables using the cor function, but let&#39;s investigate only 4 of them:</span>
<span class="kw">round</span>(<span class="kw">cor</span>(Boston[,<span class="kw">c</span>(<span class="st">&#39;medv&#39;</span>,<span class="st">&#39;lstat&#39;</span>,<span class="st">&#39;nox&#39;</span>,<span class="st">&#39;zn&#39;</span>)]),<span class="dv">3</span>)</code></pre></div>
<pre><code>##         medv  lstat    nox     zn
## medv   1.000 -0.738 -0.427  0.360
## lstat -0.738  1.000  0.591 -0.413
## nox   -0.427  0.591  1.000 -0.517
## zn     0.360 -0.413 -0.517  1.000</code></pre>
<p><br></p>
<ul>
<li>You can see there the diagonal of the matrix is all 1s. This is because a variable against itself is always going to have a perfectly correlation.</li>
<li>If you check the line of best fit between <strong>medv</strong> and <strong>lstat</strong>, we have a slight downwards slope. This behaviour is indicated by a strong negative correlation between both variables. This can be interpreted as, the more percent of households with lower status, the lower the median house value.</li>
<li>On the other hand, we can see a weaker but positive relation between <strong>medv</strong> and <strong>zn</strong> (average number of rooms). In this case, even though the relationship is weaker, a increase in the number of rooms would lead to a higher price (which is quite logical).</li>
</ul>
<p><br></p>
</div>
<div id="first-simple-linear-model-and-coefficient-interpretation" class="section level3">
<h3><span class="header-section-number">1.1.3</span> First simple linear model and coefficient interpretation</h3>
<p><br></p>
<p>So, after understanding that we have 1 possible line of best fit, and acknowledging that, unless visual checks, we need correlations to understand how are 2 variables related, we are left with the final question. How does much does an independent variable affect the dependent variable. So far we know that if correlations are negative, an increase of the independent variable will translate in a decrease of the dependent variable. But, having, for example, 2 different correlations, let’s say -0.25 and -0.80, how much do each of these relationships affect the independent variable.</p>
<p>Let’s use the previous variables used in the correlation table to get an initial first interpreation of a simple linear model. We know that <strong>lstat</strong> and <strong>nox</strong> had negative correlations, but magnitudes differ (-0.73 and -0.42). We will build 2 simple linear models and check the coefficients that will make the final formula.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># LINEAR MODEL 1 - USING LSTAT</span>
lm1 =<span class="st"> </span><span class="kw">lm</span>(medv ~<span class="st"> </span>lstat, <span class="dt">data =</span> Boston)
<span class="kw">summary</span>(lm1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.168  -3.990  -1.318   2.034  24.500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
## lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.216 on 504 degrees of freedom
## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 
## F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_line =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste0</span>(<span class="st">&quot;medv ~ &quot;</span>,
                              <span class="kw">round</span>(<span class="kw">coefficients</span>(lm1)[<span class="dv">1</span>],<span class="dv">2</span>), <span class="st">&quot; + &quot;</span>, 
                         <span class="kw">paste</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f * %s&quot;</span>, 
                                       <span class="kw">coefficients</span>(lm1)[-<span class="dv">1</span>],  
                                       <span class="kw">names</span>(<span class="kw">coefficients</span>(lm1)[-<span class="dv">1</span>])), 
                               <span class="dt">collapse=</span><span class="st">&quot; + &quot;</span>)
                        )
                      )

best_line</code></pre></div>
<pre><code>## medv ~ 34.55 + -0.95 * lstat</code></pre>
<p><br></p>
<p>So, <strong>lstat</strong> had a negative correlation of -0.73. When we apply the linear model, the line of best fit follows the formula <span class="math inline">\(medv = 34.55 + -0.95 * lstat\)</span>. The reading of this model is that, for an increase of 1 unit in <strong>lstat</strong>, <strong>medv</strong> will be decreased by 0.95. There is a lot more information in the summary of the model, but let’s leave it here and check what happens with <strong>nox</strong>.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># LINEAR MODEL 2 - USING NOX</span>
lm1 =<span class="st"> </span><span class="kw">lm</span>(medv ~<span class="st"> </span>nox, <span class="dt">data =</span> Boston)
<span class="kw">summary</span>(lm1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = medv ~ nox, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.691  -5.121  -2.161   2.959  31.310 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   41.346      1.811   22.83   &lt;2e-16 ***
## nox          -33.916      3.196  -10.61   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.323 on 504 degrees of freedom
## Multiple R-squared:  0.1826, Adjusted R-squared:  0.181 
## F-statistic: 112.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_line =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste0</span>(<span class="st">&quot;medv ~ &quot;</span>,
                              <span class="kw">round</span>(<span class="kw">coefficients</span>(lm1)[<span class="dv">1</span>],<span class="dv">2</span>), <span class="st">&quot; + &quot;</span>, 
                         <span class="kw">paste</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f * %s&quot;</span>, 
                                       <span class="kw">coefficients</span>(lm1)[-<span class="dv">1</span>],  
                                       <span class="kw">names</span>(<span class="kw">coefficients</span>(lm1)[-<span class="dv">1</span>])), 
                               <span class="dt">collapse=</span><span class="st">&quot; + &quot;</span>)
                        )
                      )

best_line</code></pre></div>
<pre><code>## medv ~ 41.35 + -33.92 * nox</code></pre>
<p><br></p>
<p>In this case, being the relationship between <strong>nox</strong> and <strong>med</strong> weaker than the one between <strong>lstat</strong> and <strong>medv</strong>, the magnitude of how <strong>medv</strong> is affected is much larger.</p>
<p>To understand this, let’s quickly plot both relationships. From the graphs below, you can see that <strong>nox</strong> has a much different range of values than <strong>lstat</strong>!!</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># LSTAT</span>
lm1 =<span class="st"> </span><span class="kw">lm</span>(medv ~<span class="st"> </span>lstat, <span class="dt">data =</span> Boston)

p1 =<span class="st">  </span>Boston %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> ~<span class="st"> </span>lstat) %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw">add_markers</span>(<span class="dt">y =</span> ~<span class="st"> </span>medv, <span class="dt">name =</span> <span class="st">&#39;medv vs lstat&#39;</span>) %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw">add_lines</span>(<span class="dt">x =</span> ~<span class="st"> </span>lstat, <span class="dt">y =</span> <span class="kw">fitted</span>(lm1), <span class="dt">name =</span> <span class="st">&#39;Best fit&#39;</span>, <span class="dt">line =</span> <span class="kw">list</span>(<span class="dt">color =</span> <span class="st">&#39;green&#39;</span>)) %&gt;%<span class="st"> </span><span class="co"># best line</span>
<span class="st">      </span><span class="kw">layout</span>(<span class="dt">title =</span> <span class="st">&#39;lstat vs medv&#39;</span>,
                 <span class="dt">xaxis =</span> <span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;lstat&quot;</span>),
                 <span class="dt">yaxis =</span> <span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;medv&quot;</span>))


<span class="co"># NOX</span>
lm2 =<span class="st"> </span><span class="kw">lm</span>(medv ~<span class="st"> </span>nox, <span class="dt">data =</span> Boston)

p2 =<span class="st">  </span>Boston %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> ~<span class="st"> </span>nox) %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw">add_markers</span>(<span class="dt">y =</span> ~<span class="st"> </span>medv, <span class="dt">name =</span> <span class="st">&#39;medv vs nox&#39;</span>) %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw">add_lines</span>(<span class="dt">x =</span> ~<span class="st"> </span>nox, <span class="dt">y =</span> <span class="kw">fitted</span>(lm2), <span class="dt">name =</span> <span class="st">&#39;Best fit&#39;</span>, <span class="dt">line =</span> <span class="kw">list</span>(<span class="dt">color =</span> <span class="st">&#39;green&#39;</span>)) %&gt;%<span class="st"> </span><span class="co"># best line</span>
<span class="st">      </span><span class="kw">layout</span>(<span class="dt">title =</span> <span class="st">&#39;nox vs medv&#39;</span>,
                 <span class="dt">xaxis =</span> <span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;nox&quot;</span>),
                 <span class="dt">yaxis =</span> <span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;medv&quot;</span>))
  
p1</code></pre></div>
<div id="79c19df7281" style="width:864px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="79c19df7281">{"x":{"visdat":{"79c19d2488":["function () ","plotlyVisDat"]},"cur_data":"79c19d2488","attrs":{"79c19d2488":{"x":{},"alpha":1,"sizes":[10,100],"y":{},"type":"scatter","mode":"markers","name":"medv vs lstat"},"79c19d2488.1":{"x":{},"alpha":1,"sizes":[10,100],"y":[29.8225950976687,25.8703897860351,30.7251419837384,31.7606957793346,29.490077823853,29.604083746304,22.7447274121713,16.3603957549176,6.11886372140643,18.3079969301215,15.1253315950322,21.9466859550146,19.6285655318451,26.7064332173421,24.8063345098261,26.5069228530529,28.3025161316555,20.6166168597534,23.4477639339522,23.837284168993,14.5838034633901,21.4146583169101,16.7689169770335,15.6668597266742,19.0680364131278,18.8685260488387,20.4836099502273,18.136988046445,22.3932091512808,23.1722496213624,13.0827254844525,22.1651973063789,8.22797328674917,17.120435237924,15.2298370239456,25.3573631350057,23.7137777530044,26.2219080469255,24.9298409258146,30.4496276711486,32.6727431589423,29.9556020071944,29.0340541340492,27.4854736874236,25.4808695509943,24.853836977514,21.1106425237075,16.6929130287329,5.2828202900994,19.1630413485036,21.7756770713381,25.5948754734452,29.5375802915409,26.5449248272032,20.4931104437648,29.9841034878072,29.0720561081995,30.801145932039,28.0365023126033,25.7943858377344,22.0606918774655,20.8351282111177,28.1600087285918,25.5283720186822,26.9059435816313,30.1171103973333,24.8253354969013,26.8584411139434,22.117694838691,26.2029070598504,28.1695092221294,25.1673532642541,29.309568446639,27.3904687520478,28.1125062609039,26.0603996567867,23.1817501148999,24.7968340162885,22.8302318540095,25.9083917601854,29.5280797980033,27.6944845452504,28.1695092221294,27.4189702326606,25.4143660962312,28.3500185993434,22.3362061900553,26.5354243336657,29.3285694337141,29.1385595629625,26.1839060727752,26.7634361785676,26.8014381527179,28.654034392546,24.492818223086,28.2360126768925,23.7802812077675,30.554133100062,31.1621646864671,28.6730353796211,25.6043759669828,27.2669623360593,24.4548162489357,21.7851775648757,22.8397323475471,18.906528022989,16.825919938259,21.167645484933,22.8967353087726,19.7805734284463,22.2031992805292,24.9013394452019,19.1535408549661,18.317497423659,24.6258251326121,19.5810630641572,23.1152466601369,24.7683325356758,19.9515823121228,21.6236691747368,20.9016316658808,20.9966366012566,17.5194559665023,10.4130868003926,17.8519732403176,20.4836099502273,8.65549549594027,18.2224924882832,19.9325813250476,17.1299357314615,22.5832190220324,22.9062358023101,23.9892920655942,20.2745990924005,18.1084865658323,18.4410038396476,18.4980068008731,20.692620808054,14.2987886572627,17.0159298090106,11.60064849259,1.86264261657064,9.07351721159378,9.45353695309698,6.72689530781155,8.14246884491095,18.7355191393126,6.49888346290963,7.6484431809568,14.1752822412742,21.1581449913954,21.937185461477,23.0392427118363,19.5525615835444,20.1890946505623,20.2840995859381,19.2200443097291,30.1931143456339,28.4450235347192,27.5329761551115,29.3285694337141,32.9102554973818,32.7297461201678,31.3996770249066,23.4952664016401,25.2338567190172,31.0386582704785,23.0202417247611,24.0082930526694,23.7992821948426,20.8446287046553,23.1247471536745,20.5976158726782,25.9653947214109,25.3953651091561,29.490077823853,24.9488419128898,28.5780304442453,27.9794993513778,29.7655921364428,27.3714677649727,25.5758744863701,29.9746029942696,29.1575605500377,21.2721509138464,22.0606918774655,30.32612125516,28.2075111962797,30.2216158262467,29.4330748626275,29.7085891752173,30.0981094102581,31.8271992340977,29.7750926299804,30.3926247099231,31.7321942987219,30.6776395160505,26.3739159435268,28.2645141575052,30.2216158262467,30.32612125516,27.4949741809612,31.5991873891957,30.9341528415652,31.8176987405601,24.2268044040337,24.1317994686579,17.3959495505138,20.626117353291,12.6172013011111,18.1464885399826,11.7716573762665,19.3245497386425,25.6423779411331,6.47988247583447,25.5568734992949,21.7186741101126,25.3478626414682,17.5289564600399,24.5783226649242,25.328861654393,14.1657817477366,25.1198507965663,27.3334657908224,30.620636554825,30.1551123714836,31.5801864021206,28.5115269894823,30.8296474126518,30.9816553092531,23.4857659081025,29.5660817721536,32.2072189756009,30.801145932039,26.9059435816313,24.2173039104962,25.4903700445319,30.0601074361078,28.5115269894823,27.5519771421867,23.7422792336172,22.773228892784,23.8942871302184,29.6230847333791,22.6782239574082,17.0159298090106,25.8513887989599,24.9108399387395,25.509371031607,28.3215171187307,28.9485496922109,31.1431636993919,31.2001666606174,31.1906661670798,28.3120166251931,25.7658843571217,31.5991873891957,29.6895881881422,27.1529564136084,27.998500338453,25.442867576844,27.6564825711001,28.9390491986734,23.8657856496057,26.8584411139434,24.6258251326121,20.5026109373024,27.4854736874236,31.5516849215078,21.5856672005865,22.2031992805292,28.2930156381179,27.2099593748338,28.3025161316555,31.2001666606174,31.7226938051843,28.8060422891473,30.6016355677499,27.7229860258631,29.9461015136568,30.981655309253,30.1931143456339,31.6941923245715,31.5516849215078,27.0959534523829,26.7349346979548,22.2697027352923,27.770488493551,27.3334657908224,25.5188715251446,31.390176531369,31.1716651800046,30.0886089167205,26.4024174241395,24.6733276003,28.5970314313205,27.5329761551115,19.5050591158565,29.8320955912059,30.0506069425702,28.7870413020721,25.5283720186822,26.3169129823013,29.9366010201193,27.9699988578402,26.0699001503242,28.4070215605689,27.3999692455854,30.2406168133218,25.0818488224159,22.5452170478821,28.8725457439103,23.4192624533394,27.048450984695,25.7373828765089,23.6282733111662,17.1394362249991,19.4100541804807,24.7113295744503,22.4597126060439,27.7134855323256,28.0270018190657,27.2384608554466,23.4002614662643,28.7395388343842,29.7275901622925,28.7110373537715,22.4027096448184,25.0818488224159,27.5804786227994,25.917892253723,22.7447274121713,27.114954439458,29.1575605500377,28.1410077415167,26.9439455557816,25.2433572125548,24.5213197036987,26.4689208789026,25.3003601737803,25.7278823829714,29.3380699272517,26.3359139693765,27.7324865194007,30.1741133585588,24.5498211843115,22.5167155672694,28.5115269894823,28.8630452503727,28.9580501857485,28.8725457439103,29.3380699272517,27.1529564136084,30.2786187874721,26.9059435816313,29.2620659789511,17.8329722532425,21.9466859550146,23.6472742982414,22.5167155672694,27.1529564136083,21.0726405495572,24.8728379645892,20.6451183403661,29.5280797980033,27.7894894806262,21.2531499267712,21.8896829937891,31.456679986132,31.0101567898658,31.7416947922594,25.4998705380694,26.1174026180121,1.52062484921775,-1.51953308280781,21.7851775648757,12.4746938980474,14.3747926055634,12.0471716888563,13.8617659545341,18.2034915012081,14.5268005021647,12.1326761306945,11.2206287510868,5.45382917377584,5.28282029009941,7.68644515510712,4.16176205266497,5.46332966731342,14.745311853529,18.2984964365839,16.7309150028832,10.1565734748779,20.1415921828744,19.0205339454399,18.2889959430463,16.1513848970908,15.6288577525239,5.49183114792616,6.08086174725611,9.12101967928169,15.2488380110207,15.2583385045583,15.7713651555876,8.54148957348931,12.7217067300245,12.3796889626716,23.0297422182987,9.47253794017214,15.76186466205,24.9488419128898,14.3937935926385,1.90064459072096,15.4768498559226,-0.578984222587397,6.95490715271347,10.0520680459645,9.24452609527022,14.9638232048933,12.9497185749264,20.2840995859381,19.6380660253826,21.1581449913954,12.4271914303595,18.250993968896,11.3821371412257,19.6475665189202,20.7591242628171,14.1087787865111,11.6766524408907,17.7949702790921,15.8473691038883,23.1247471536745,19.1440403614285,20.1415921828744,12.4461924174347,17.4054500440514,9.42503547248424,2.23316186453625,12.8167116654003,13.5482496677939,16.0088774940271,18.792522100538,16.645410561045,11.9521667534805,11.7716573762665,17.6524628760284,18.9350295036017,17.3294460957507,16.2083878583163,17.9849801498437,17.7094658372539,18.1464885399826,18.6500146974743,16.7784174705711,17.3294460957507,16.4934026644437,18.4600048267227,19.1345398678909,20.5881153791406,18.9540304906769,20.6356178468285,21.2626504203088,24.7778330292134,21.9941884227025,21.1296435107827,18.2604944624336,14.2987886572627,17.3294460957507,20.5311124179152,19.0775369066654,22.3267056965178,20.9111321594184,23.4762654145649,17.3199456022131,11.6576514538155,16.8069189511838,10.8881114772715,17.4244510311265,22.0986938516158,24.3503108200223,27.2004588812962,27.8939949095396,24.6543266132248,21.8801825002515,24.5023187166236,20.3221015600884,23.6757757788541,17.3959495505138,11.7811578698041,6.35637605984593,17.3864490569762,21.8706820067139,23.1437481407496,21.642670161812,17.8329722532425,14.4697975409392,21.1581449913954,22.2792032288299,20.2080956376374,20.9396336400311,25.3668636285433,25.9273927472605,29.195562524188,28.3975210670313,27.0674519717701],"type":"scatter","mode":"lines","name":"Best fit","line":{"color":"green"}}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"lstat vs medv","xaxis":{"domain":[0,1],"title":"lstat"},"yaxis":{"domain":[0,1],"title":"medv"},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[4.98,9.14,4.03,2.94,5.33,5.21,12.43,19.15,29.93,17.1,20.45,13.27,15.71,8.26,10.26,8.47,6.58,14.67,11.69,11.28,21.02,13.83,18.72,19.88,16.3,16.51,14.81,17.28,12.8,11.98,22.6,13.04,27.71,18.35,20.34,9.68,11.41,8.77,10.13,4.32,1.98,4.84,5.81,7.44,9.55,10.21,14.15,18.8,30.81,16.2,13.45,9.43,5.28,8.43,14.8,4.81,5.77,3.95,6.86,9.22,13.15,14.44,6.73,9.5,8.05,4.67,10.24,8.1,13.09,8.79,6.72,9.88,5.52,7.54,6.78,8.94,11.97,10.27,12.34,9.1,5.29,7.22,6.72,7.51,9.62,6.53,12.86,8.44,5.5,5.7,8.81,8.2,8.16,6.21,10.59,6.65,11.34,4.21,3.57,6.19,9.42,7.67,10.63,13.44,12.33,16.47,18.66,14.09,12.27,15.55,13,10.16,16.21,17.09,10.45,15.76,12.04,10.3,15.37,13.61,14.37,14.27,17.93,25.41,17.58,14.81,27.26,17.19,15.39,18.34,12.6,12.26,11.12,15.03,17.31,16.96,16.9,14.59,21.32,18.46,24.16,34.41,26.82,26.42,29.29,27.8,16.65,29.53,28.32,21.45,14.1,13.28,12.12,15.79,15.12,15.02,16.14,4.59,6.43,7.39,5.5,1.73,1.92,3.32,11.64,9.81,3.7,12.14,11.1,11.32,14.43,12.03,14.69,9.04,9.64,5.33,10.11,6.29,6.92,5.04,7.56,9.45,4.82,5.68,13.98,13.15,4.45,6.68,4.56,5.39,5.1,4.69,2.87,5.03,4.38,2.97,4.08,8.61,6.62,4.56,4.45,7.43,3.11,3.81,2.88,10.87,10.97,18.06,14.66,23.09,17.27,23.98,16.03,9.38,29.55,9.47,13.51,9.69,17.92,10.5,9.71,21.46,9.93,7.6,4.14,4.63,3.13,6.36,3.92,3.76,11.65,5.25,2.47,3.95,8.05,10.88,9.54,4.73,6.36,7.37,11.38,12.4,11.22,5.19,12.5,18.46,9.16,10.15,9.52,6.56,5.9,3.59,3.53,3.54,6.57,9.25,3.11,5.12,7.79,6.9,9.59,7.26,5.91,11.25,8.1,10.45,14.79,7.44,3.16,13.65,13,6.59,7.73,6.58,3.53,2.98,6.05,4.16,7.19,4.85,3.76,4.59,3.01,3.16,7.85,8.23,12.93,7.14,7.6,9.51,3.33,3.56,4.7,8.58,10.4,6.27,7.39,15.84,4.97,4.74,6.07,9.5,8.67,4.86,6.93,8.93,6.47,7.53,4.54,9.97,12.64,5.98,11.72,7.9,9.28,11.5,18.33,15.94,10.36,12.73,7.2,6.87,7.7,11.74,6.12,5.08,6.15,12.79,9.97,7.34,9.09,12.43,7.83,5.68,6.75,8.01,9.8,10.56,8.51,9.74,9.29,5.49,8.65,7.18,4.61,10.53,12.67,6.36,5.99,5.89,5.98,5.49,7.79,4.5,8.05,5.57,17.6,13.27,11.48,12.67,7.79,14.19,10.19,14.64,5.29,7.12,14,13.33,3.26,3.73,2.96,9.53,8.88,34.77,37.97,13.44,23.24,21.24,23.69,21.78,17.21,21.08,23.6,24.56,30.63,30.81,28.28,31.99,30.62,20.85,17.11,18.76,25.68,15.17,16.35,17.12,19.37,19.92,30.59,29.97,26.77,20.32,20.31,19.77,27.38,22.98,23.34,12.13,26.4,19.78,10.11,21.22,34.37,20.08,36.98,29.05,25.79,26.64,20.62,22.74,15.02,15.7,14.1,23.29,17.16,24.39,15.69,14.52,21.52,24.08,17.64,19.69,12.03,16.22,15.17,23.27,18.05,26.45,34.02,22.88,22.11,19.52,16.59,18.85,23.79,23.98,17.79,16.44,18.13,19.31,17.44,17.73,17.27,16.74,18.71,18.13,19.01,16.94,16.23,14.7,16.42,14.65,13.99,10.29,13.22,14.13,17.15,21.32,18.13,14.76,16.29,12.87,14.36,11.66,18.14,24.1,18.68,24.91,18.03,13.11,10.74,7.74,7.01,10.42,13.34,10.58,14.98,11.45,18.06,23.97,29.68,18.07,13.35,12.01,13.59,17.6,21.14,14.1,12.92,15.1,14.33,9.67,9.08,5.64,6.48,7.88],"y":[24,21.6,34.7,33.4,36.2,28.7,22.9,27.1,16.5,18.9,15,18.9,21.7,20.4,18.2,19.9,23.1,17.5,20.2,18.2,13.6,19.6,15.2,14.5,15.6,13.9,16.6,14.8,18.4,21,12.7,14.5,13.2,13.1,13.5,18.9,20,21,24.7,30.8,34.9,26.6,25.3,24.7,21.2,19.3,20,16.6,14.4,19.4,19.7,20.5,25,23.4,18.9,35.4,24.7,31.6,23.3,19.6,18.7,16,22.2,25,33,23.5,19.4,22,17.4,20.9,24.2,21.7,22.8,23.4,24.1,21.4,20,20.8,21.2,20.3,28,23.9,24.8,22.9,23.9,26.6,22.5,22.2,23.6,28.7,22.6,22,22.9,25,20.6,28.4,21.4,38.7,43.8,33.2,27.5,26.5,18.6,19.3,20.1,19.5,19.5,20.4,19.8,19.4,21.7,22.8,18.8,18.7,18.5,18.3,21.2,19.2,20.4,19.3,22,20.3,20.5,17.3,18.8,21.4,15.7,16.2,18,14.3,19.2,19.6,23,18.4,15.6,18.1,17.4,17.1,13.3,17.8,14,14.4,13.4,15.6,11.8,13.8,15.6,14.6,17.8,15.4,21.5,19.6,15.3,19.4,17,15.6,13.1,41.3,24.3,23.3,27,50,50,50,22.7,25,50,23.8,23.8,22.3,17.4,19.1,23.1,23.6,22.6,29.4,23.2,24.6,29.9,37.2,39.8,36.2,37.9,32.5,26.4,29.6,50,32,29.8,34.9,37,30.5,36.4,31.1,29.1,50,33.3,30.3,34.6,34.9,32.9,24.1,42.3,48.5,50,22.6,24.4,22.5,24.4,20,21.7,19.3,22.4,28.1,23.7,25,23.3,28.7,21.5,23,26.7,21.7,27.5,30.1,44.8,50,37.6,31.6,46.7,31.5,24.3,31.7,41.7,48.3,29,24,25.1,31.5,23.7,23.3,22,20.1,22.2,23.7,17.6,18.5,24.3,20.5,24.5,26.2,24.4,24.8,29.6,42.8,21.9,20.9,44,50,36,30.1,33.8,43.1,48.8,31,36.5,22.8,30.7,50,43.5,20.7,21.1,25.2,24.4,35.2,32.4,32,33.2,33.1,29.1,35.1,45.4,35.4,46,50,32.2,22,20.1,23.2,22.3,24.8,28.5,37.3,27.9,23.9,21.7,28.6,27.1,20.3,22.5,29,24.8,22,26.4,33.1,36.1,28.4,33.4,28.2,22.8,20.3,16.1,22.1,19.4,21.6,23.8,16.2,17.8,19.8,23.1,21,23.8,23.1,20.4,18.5,25,24.6,23,22.2,19.3,22.6,19.8,17.1,19.4,22.2,20.7,21.1,19.5,18.5,20.6,19,18.7,32.7,16.5,23.9,31.2,17.5,17.2,23.1,24.5,26.6,22.9,24.1,18.6,30.1,18.2,20.6,17.8,21.7,22.7,22.6,25,19.9,20.8,16.8,21.9,27.5,21.9,23.1,50,50,50,50,50,13.8,13.8,15,13.9,13.3,13.1,10.2,10.4,10.9,11.3,12.3,8.8,7.2,10.5,7.4,10.2,11.5,15.1,23.2,9.7,13.8,12.7,13.1,12.5,8.5,5,6.3,5.6,7.2,12.1,8.3,8.5,5,11.9,27.9,17.2,27.5,15,17.2,17.9,16.3,7,7.2,7.5,10.4,8.8,8.4,16.7,14.2,20.8,13.4,11.7,8.3,10.2,10.9,11,9.5,14.5,14.1,16.1,14.3,11.7,13.4,9.6,8.7,8.4,12.8,10.5,17.1,18.4,15.4,10.8,11.8,14.9,12.6,14.1,13,13.4,15.2,16.1,17.8,14.9,14.1,12.7,13.5,14.9,20,16.4,17.7,19.5,20.2,21.4,19.9,19,19.1,19.1,20.1,19.9,19.6,23.2,29.8,13.8,13.3,16.7,12,14.6,21.4,23,23.7,25,21.8,20.6,21.2,19.1,20.6,15.2,7,8.1,13.6,20.1,21.8,24.5,23.1,19.7,18.3,21.2,17.5,16.8,22.4,20.6,23.9,22,11.9],"type":"scatter","mode":"markers","name":"medv vs lstat","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(31,119,180,1)","line":{"color":"transparent"}},"xaxis":"x","yaxis":"y","frame":null},{"x":[1.73,1.92,1.98,2.47,2.87,2.88,2.94,2.96,2.97,2.98,3.01,3.11,3.11,3.13,3.16,3.16,3.26,3.32,3.33,3.53,3.53,3.54,3.56,3.57,3.59,3.7,3.73,3.76,3.76,3.81,3.92,3.95,3.95,4.03,4.08,4.14,4.16,4.21,4.32,4.38,4.45,4.45,4.5,4.54,4.56,4.56,4.59,4.59,4.61,4.63,4.67,4.69,4.7,4.73,4.74,4.81,4.82,4.84,4.85,4.86,4.97,4.98,5.03,5.04,5.08,5.1,5.12,5.19,5.21,5.25,5.28,5.29,5.29,5.33,5.33,5.39,5.49,5.49,5.5,5.5,5.52,5.57,5.64,5.68,5.68,5.7,5.77,5.81,5.89,5.9,5.91,5.98,5.98,5.99,6.05,6.07,6.12,6.15,6.19,6.21,6.27,6.29,6.36,6.36,6.36,6.43,6.47,6.48,6.53,6.56,6.57,6.58,6.58,6.59,6.62,6.65,6.68,6.72,6.72,6.73,6.75,6.78,6.86,6.87,6.9,6.92,6.93,7.01,7.12,7.14,7.18,7.19,7.2,7.22,7.26,7.34,7.37,7.39,7.39,7.43,7.44,7.44,7.51,7.53,7.54,7.56,7.6,7.6,7.67,7.7,7.73,7.74,7.79,7.79,7.79,7.83,7.85,7.88,7.9,8.01,8.05,8.05,8.05,8.1,8.1,8.16,8.2,8.23,8.26,8.43,8.44,8.47,8.51,8.58,8.61,8.65,8.67,8.77,8.79,8.81,8.88,8.93,8.94,9.04,9.08,9.09,9.1,9.14,9.16,9.22,9.25,9.28,9.29,9.38,9.42,9.43,9.45,9.47,9.5,9.5,9.51,9.52,9.53,9.54,9.55,9.59,9.62,9.64,9.67,9.68,9.69,9.71,9.74,9.8,9.81,9.88,9.93,9.97,9.97,10.11,10.11,10.13,10.15,10.16,10.19,10.21,10.24,10.26,10.27,10.29,10.3,10.36,10.4,10.42,10.45,10.45,10.5,10.53,10.56,10.58,10.59,10.63,10.74,10.87,10.88,10.97,11.1,11.12,11.22,11.25,11.28,11.32,11.34,11.38,11.41,11.45,11.48,11.5,11.64,11.65,11.66,11.69,11.72,11.74,11.97,11.98,12.01,12.03,12.03,12.04,12.12,12.13,12.14,12.26,12.27,12.33,12.34,12.4,12.43,12.43,12.5,12.6,12.64,12.67,12.67,12.73,12.79,12.8,12.86,12.87,12.92,12.93,13,13,13.04,13.09,13.11,13.15,13.15,13.22,13.27,13.27,13.28,13.33,13.34,13.35,13.44,13.44,13.45,13.51,13.59,13.61,13.65,13.83,13.98,13.99,14,14.09,14.1,14.1,14.1,14.13,14.15,14.19,14.27,14.33,14.36,14.37,14.43,14.44,14.52,14.59,14.64,14.65,14.66,14.67,14.69,14.7,14.76,14.79,14.8,14.81,14.81,14.98,15.02,15.02,15.03,15.1,15.12,15.17,15.17,15.37,15.39,15.55,15.69,15.7,15.71,15.76,15.79,15.84,15.94,16.03,16.14,16.2,16.21,16.22,16.23,16.29,16.3,16.35,16.42,16.44,16.47,16.51,16.59,16.65,16.74,16.9,16.94,16.96,17.09,17.1,17.11,17.12,17.15,17.16,17.19,17.21,17.27,17.27,17.28,17.31,17.44,17.58,17.6,17.6,17.64,17.73,17.79,17.92,17.93,18.03,18.05,18.06,18.06,18.07,18.13,18.13,18.13,18.14,18.33,18.34,18.35,18.46,18.46,18.66,18.68,18.71,18.72,18.76,18.8,18.85,19.01,19.15,19.31,19.37,19.52,19.69,19.77,19.78,19.88,19.92,20.08,20.31,20.32,20.34,20.45,20.62,20.85,21.02,21.08,21.14,21.22,21.24,21.32,21.32,21.45,21.46,21.52,21.78,22.11,22.6,22.74,22.88,22.98,23.09,23.24,23.27,23.29,23.34,23.6,23.69,23.79,23.97,23.98,23.98,24.08,24.1,24.16,24.39,24.56,24.91,25.41,25.68,25.79,26.4,26.42,26.45,26.64,26.77,26.82,27.26,27.38,27.71,27.8,28.28,28.32,29.05,29.29,29.53,29.55,29.68,29.93,29.97,30.59,30.62,30.63,30.81,30.81,31.99,34.02,34.37,34.41,34.77,36.98,37.97],"y":[32.9102554973818,32.7297461201678,32.6727431589423,32.2072189756009,31.8271992340977,31.8176987405601,31.7606957793346,31.7416947922594,31.7321942987219,31.7226938051843,31.6941923245715,31.5991873891957,31.5991873891957,31.5801864021206,31.5516849215078,31.5516849215078,31.456679986132,31.3996770249066,31.390176531369,31.2001666606174,31.2001666606174,31.1906661670798,31.1716651800046,31.1621646864671,31.1431636993919,31.0386582704785,31.0101567898658,30.9816553092531,30.981655309253,30.9341528415652,30.8296474126518,30.801145932039,30.801145932039,30.7251419837384,30.6776395160505,30.620636554825,30.6016355677499,30.554133100062,30.4496276711486,30.3926247099231,30.32612125516,30.32612125516,30.2786187874721,30.2406168133218,30.2216158262467,30.2216158262467,30.1931143456339,30.1931143456339,30.1741133585588,30.1551123714836,30.1171103973333,30.0981094102581,30.0886089167205,30.0601074361078,30.0506069425702,29.9841034878072,29.9746029942696,29.9556020071944,29.9461015136568,29.9366010201193,29.8320955912059,29.8225950976687,29.7750926299804,29.7655921364428,29.7275901622925,29.7085891752173,29.6895881881422,29.6230847333791,29.604083746304,29.5660817721536,29.5375802915409,29.5280797980033,29.5280797980033,29.490077823853,29.490077823853,29.4330748626275,29.3380699272517,29.3380699272517,29.3285694337141,29.3285694337141,29.309568446639,29.2620659789511,29.195562524188,29.1575605500377,29.1575605500377,29.1385595629625,29.0720561081995,29.0340541340492,28.9580501857485,28.9485496922109,28.9390491986734,28.8725457439103,28.8725457439103,28.8630452503727,28.8060422891473,28.7870413020721,28.7395388343842,28.7110373537715,28.6730353796211,28.654034392546,28.5970314313205,28.5780304442453,28.5115269894823,28.5115269894823,28.5115269894823,28.4450235347192,28.4070215605689,28.3975210670313,28.3500185993434,28.3215171187307,28.3120166251931,28.3025161316555,28.3025161316555,28.2930156381179,28.2645141575052,28.2360126768925,28.2075111962797,28.1695092221294,28.1695092221294,28.1600087285918,28.1410077415167,28.1125062609039,28.0365023126033,28.0270018190657,27.998500338453,27.9794993513778,27.9699988578402,27.8939949095396,27.7894894806262,27.770488493551,27.7324865194007,27.7229860258631,27.7134855323256,27.6944845452504,27.6564825711001,27.5804786227994,27.5519771421867,27.5329761551115,27.5329761551115,27.4949741809612,27.4854736874236,27.4854736874236,27.4189702326606,27.3999692455854,27.3904687520478,27.3714677649727,27.3334657908224,27.3334657908224,27.2669623360593,27.2384608554466,27.2099593748338,27.2004588812962,27.1529564136084,27.1529564136084,27.1529564136083,27.114954439458,27.0959534523829,27.0674519717701,27.048450984695,26.9439455557816,26.9059435816313,26.9059435816313,26.9059435816313,26.8584411139434,26.8584411139434,26.8014381527179,26.7634361785676,26.7349346979548,26.7064332173421,26.5449248272032,26.5354243336657,26.5069228530529,26.4689208789026,26.4024174241395,26.3739159435268,26.3359139693765,26.3169129823013,26.2219080469255,26.2029070598504,26.1839060727752,26.1174026180121,26.0699001503242,26.0603996567867,25.9653947214109,25.9273927472605,25.917892253723,25.9083917601854,25.8703897860351,25.8513887989599,25.7943858377344,25.7658843571217,25.7373828765089,25.7278823829714,25.6423779411331,25.6043759669828,25.5948754734452,25.5758744863701,25.5568734992949,25.5283720186822,25.5283720186822,25.5188715251446,25.509371031607,25.4998705380694,25.4903700445319,25.4808695509943,25.442867576844,25.4143660962312,25.3953651091561,25.3668636285433,25.3573631350057,25.3478626414682,25.328861654393,25.3003601737803,25.2433572125548,25.2338567190172,25.1673532642541,25.1198507965663,25.0818488224159,25.0818488224159,24.9488419128898,24.9488419128898,24.9298409258146,24.9108399387395,24.9013394452019,24.8728379645892,24.853836977514,24.8253354969013,24.8063345098261,24.7968340162885,24.7778330292134,24.7683325356758,24.7113295744503,24.6733276003,24.6543266132248,24.6258251326121,24.6258251326121,24.5783226649242,24.5498211843115,24.5213197036987,24.5023187166236,24.492818223086,24.4548162489357,24.3503108200223,24.2268044040337,24.2173039104962,24.1317994686579,24.0082930526694,23.9892920655942,23.8942871302184,23.8657856496057,23.837284168993,23.7992821948426,23.7802812077675,23.7422792336172,23.7137777530044,23.6757757788541,23.6472742982414,23.6282733111662,23.4952664016401,23.4857659081025,23.4762654145649,23.4477639339522,23.4192624533394,23.4002614662643,23.1817501148999,23.1722496213624,23.1437481407496,23.1247471536745,23.1247471536745,23.1152466601369,23.0392427118363,23.0297422182987,23.0202417247611,22.9062358023101,22.8967353087726,22.8397323475471,22.8302318540095,22.773228892784,22.7447274121713,22.7447274121713,22.6782239574082,22.5832190220324,22.5452170478821,22.5167155672694,22.5167155672694,22.4597126060439,22.4027096448184,22.3932091512808,22.3362061900553,22.3267056965178,22.2792032288299,22.2697027352923,22.2031992805292,22.2031992805292,22.1651973063789,22.117694838691,22.0986938516158,22.0606918774655,22.0606918774655,21.9941884227025,21.9466859550146,21.9466859550146,21.937185461477,21.8896829937891,21.8801825002515,21.8706820067139,21.7851775648757,21.7851775648757,21.7756770713381,21.7186741101126,21.642670161812,21.6236691747368,21.5856672005865,21.4146583169101,21.2721509138464,21.2626504203088,21.2531499267712,21.167645484933,21.1581449913954,21.1581449913954,21.1581449913954,21.1296435107827,21.1106425237075,21.0726405495572,20.9966366012566,20.9396336400311,20.9111321594184,20.9016316658808,20.8446287046553,20.8351282111177,20.7591242628171,20.692620808054,20.6451183403661,20.6356178468285,20.626117353291,20.6166168597534,20.5976158726782,20.5881153791406,20.5311124179152,20.5026109373024,20.4931104437648,20.4836099502273,20.4836099502273,20.3221015600884,20.2840995859381,20.2840995859381,20.2745990924005,20.2080956376374,20.1890946505623,20.1415921828744,20.1415921828744,19.9515823121228,19.9325813250476,19.7805734284463,19.6475665189202,19.6380660253826,19.6285655318451,19.5810630641572,19.5525615835444,19.5050591158565,19.4100541804807,19.3245497386425,19.2200443097291,19.1630413485036,19.1535408549661,19.1440403614285,19.1345398678909,19.0775369066654,19.0680364131278,19.0205339454399,18.9540304906769,18.9350295036017,18.906528022989,18.8685260488387,18.792522100538,18.7355191393126,18.6500146974743,18.4980068008731,18.4600048267227,18.4410038396476,18.317497423659,18.3079969301215,18.2984964365839,18.2889959430463,18.2604944624336,18.250993968896,18.2224924882832,18.2034915012081,18.1464885399826,18.1464885399826,18.136988046445,18.1084865658323,17.9849801498437,17.8519732403176,17.8329722532425,17.8329722532425,17.7949702790921,17.7094658372539,17.6524628760284,17.5289564600399,17.5194559665023,17.4244510311265,17.4054500440514,17.3959495505138,17.3959495505138,17.3864490569762,17.3294460957507,17.3294460957507,17.3294460957507,17.3199456022131,17.1394362249991,17.1299357314615,17.120435237924,17.0159298090106,17.0159298090106,16.825919938259,16.8069189511838,16.7784174705711,16.7689169770335,16.7309150028832,16.6929130287329,16.645410561045,16.4934026644437,16.3603957549176,16.2083878583163,16.1513848970908,16.0088774940271,15.8473691038883,15.7713651555876,15.76186466205,15.6668597266742,15.6288577525239,15.4768498559226,15.2583385045583,15.2488380110207,15.2298370239456,15.1253315950322,14.9638232048933,14.745311853529,14.5838034633901,14.5268005021647,14.4697975409392,14.3937935926385,14.3747926055634,14.2987886572627,14.2987886572627,14.1752822412742,14.1657817477366,14.1087787865111,13.8617659545341,13.5482496677939,13.0827254844525,12.9497185749264,12.8167116654003,12.7217067300245,12.6172013011111,12.4746938980474,12.4461924174347,12.4271914303595,12.3796889626716,12.1326761306945,12.0471716888563,11.9521667534805,11.7811578698041,11.7716573762665,11.7716573762665,11.6766524408907,11.6576514538155,11.60064849259,11.3821371412257,11.2206287510868,10.8881114772715,10.4130868003926,10.1565734748779,10.0520680459645,9.47253794017214,9.45353695309698,9.42503547248424,9.24452609527022,9.12101967928169,9.07351721159378,8.65549549594027,8.54148957348931,8.22797328674917,8.14246884491095,7.68644515510712,7.6484431809568,6.95490715271347,6.72689530781155,6.49888346290963,6.47988247583447,6.35637605984593,6.11886372140643,6.08086174725611,5.49183114792616,5.46332966731342,5.45382917377584,5.2828202900994,5.28282029009941,4.16176205266497,2.23316186453625,1.90064459072096,1.86264261657064,1.52062484921775,-0.578984222587397,-1.51953308280781],"type":"scatter","mode":"lines","name":"Best fit","line":{"fillcolor":"rgba(255,127,14,1)","color":"green"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p2</code></pre></div>
<div id="79c784e3822" style="width:864px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="79c784e3822">{"x":{"visdat":{"79c272c3e57":["function () ","plotlyVisDat"]},"cur_data":"79c272c3e57","attrs":{"79c272c3e57":{"x":{},"alpha":1,"sizes":[10,100],"y":{},"type":"scatter","mode":"markers","name":"medv vs nox"},"79c272c3e57.1":{"x":{},"alpha":1,"sizes":[10,100],"y":[23.0990368733136,25.4392446689113,25.4392446689112,25.8123212740065,25.8123212740065,25.8123212740065,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,24.4217630186514,24.4217630186514,24.4217630186514,24.4217630186514,26.8298029242663,26.8298029242663,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.456726319171,26.456726319171,26.456726319171,26.456726319171,27.4402919144222,27.6777042994828,27.4402919144222,27.4063758594135,25.9819015490498,25.9819015490498,25.9819015490498,25.9819015490498,25.9819015490498,25.9819015490498,27.2334039788694,27.8472845745261,27.8472845745261,27.4742079694309,27.4742079694309,27.4742079694309,27.3385437493962,27.3385437493962,27.3385437493962,27.3385437493962,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.8976350342836,26.8976350342836,26.8976350342836,26.8976350342836,26.1175657690844,26.1175657690844,26.1175657690844,26.1175657690844,24.760923568738,24.760923568738,24.760923568738,24.760923568738,25.6088249439545,25.6088249439545,25.6088249439545,26.2532299891191,26.2532299891191,26.253229989119,26.253229989119,26.2532299891191,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,20.8266611877333,20.8266611877333,11.8049905554294,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,27.7455364095001,27.7455364095001,27.0332992543183,27.6437882444742,27.6437882444742,27.6437882444742,27.6777042994828,27.6777042994828,27.2707116393789,27.2707116393789,27.2334039788694,27.2334039788694,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,22.6920442132096,22.6920442132096,22.6920442132096,22.6920442132096,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,28.0507809045781,28.0507809045781,27.9829487945608,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,21.8441428379931,21.8441428379931,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,26.1853978791017,26.1853978791017,26.1853978791017,26.1853978791017,26.1853978791017,26.3244537046372,26.3244537046372,26.3244537046372,26.3244537046372,27.7455364095001,27.7794524645088,28.1525290696041,28.2881932896387,27.6098721894655,27.6098721894655,27.6098721894655,27.4063758594135,27.4063758594135,27.4063758594135,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,27.7794524645088,27.7794524645088,27.7794524645088,26.660222649223,26.660222649223,26.660222649223,25.3374965038852,25.3374965038852,25.3374965038852,25.3374965038852,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,25.7444891639891,25.7444891639891,25.7444891639891,26.4940339796805,26.4940339796805,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,26.354978154145,23.7773579734868,24.9305038437813,24.9305038437813,26.354978154145,26.354978154145,26.7958868692576,26.5923905392057,26.7958868692576,26.7958868692576,27.4063758594135,27.4063758594135,27.4402919144222,27.3385437493962,27.3385437493962,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,16.9941469717546,16.9941469717546,16.9941469717546,19.9448437575081,19.9448437575081,19.9448437575081,19.9448437575081,19.9448437575081,18.6899497221876,18.6899497221876,18.6899497221876,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,18.9951942172656,18.9951942172656,21.0979896278026,21.0979896278026,21.0979896278026,21.0979896278026,21.0979896278026,21.0979896278026,17.8420483469711,18.3168731170924,18.3168731170923,18.3168731170924,18.3168731170923,16.9941469717546,16.9941469717546,16.9941469717546,20.5214166926553,20.5214166926553,21.5388983429152,18.3168731170923,21.5388983429152,18.3168731170924,18.3168731170924,18.3168731170923,21.5388983429152,21.5388983429152,21.5388983429152,17.1637272467979,17.1637272467979,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,19.1308584373002,19.1308584373002,19.1308584373002,21.5388983429152,21.6745625629498,21.6745625629498,21.6745625629498,23.3025332033655,21.6745625629498,20.5214166926553,21.5388983429152,21.5388983429152,20.5214166926553,20.5214166926553,20.5214166926553,20.5214166926553,23.3025332033655,23.3025332033655,23.3025332033655,23.3025332033655,21.5728143979238,21.5728143979238,21.5728143979238,21.5728143979238,20.6909969676986,20.6909969676986,20.6909969676986,20.6909969676986,20.6909969676986,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.9119749480104,21.9119749480104,21.9119749480104,21.9119749480104,21.9119749480104],"type":"scatter","mode":"lines","name":"Best fit","line":{"color":"green"}}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"nox vs medv","xaxis":{"domain":[0,1],"title":"nox"},"yaxis":{"domain":[0,1],"title":"medv"},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[0.538,0.469,0.469,0.458,0.458,0.458,0.524,0.524,0.524,0.524,0.524,0.524,0.524,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.499,0.499,0.499,0.499,0.428,0.428,0.448,0.448,0.448,0.448,0.448,0.448,0.448,0.448,0.448,0.439,0.439,0.439,0.439,0.41,0.403,0.41,0.411,0.453,0.453,0.453,0.453,0.453,0.453,0.4161,0.398,0.398,0.409,0.409,0.409,0.413,0.413,0.413,0.413,0.437,0.437,0.437,0.437,0.437,0.437,0.426,0.426,0.426,0.426,0.449,0.449,0.449,0.449,0.489,0.489,0.489,0.489,0.464,0.464,0.464,0.445,0.445,0.445,0.445,0.445,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.547,0.547,0.547,0.547,0.547,0.547,0.547,0.547,0.547,0.581,0.581,0.581,0.581,0.581,0.581,0.581,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.605,0.605,0.871,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.51,0.51,0.51,0.51,0.51,0.51,0.51,0.488,0.488,0.488,0.488,0.488,0.488,0.488,0.488,0.437,0.437,0.437,0.437,0.437,0.437,0.401,0.401,0.422,0.404,0.404,0.404,0.403,0.403,0.415,0.415,0.4161,0.4161,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.55,0.55,0.55,0.55,0.507,0.507,0.507,0.507,0.504,0.504,0.504,0.504,0.504,0.504,0.504,0.504,0.507,0.507,0.507,0.507,0.507,0.507,0.428,0.428,0.428,0.428,0.428,0.428,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.392,0.392,0.394,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.575,0.575,0.464,0.464,0.464,0.464,0.464,0.447,0.447,0.447,0.447,0.447,0.4429,0.4429,0.4429,0.4429,0.401,0.4,0.389,0.385,0.405,0.405,0.405,0.411,0.411,0.411,0.437,0.437,0.437,0.437,0.437,0.4,0.4,0.4,0.433,0.433,0.433,0.472,0.472,0.472,0.472,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.493,0.493,0.493,0.493,0.493,0.493,0.493,0.493,0.46,0.46,0.46,0.4379,0.4379,0.515,0.515,0.515,0.515,0.515,0.515,0.515,0.515,0.442,0.518,0.484,0.484,0.442,0.442,0.429,0.435,0.429,0.429,0.411,0.411,0.41,0.413,0.413,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.718,0.718,0.718,0.631,0.631,0.631,0.631,0.631,0.668,0.668,0.668,0.671,0.671,0.671,0.671,0.671,0.671,0.671,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.659,0.659,0.597,0.597,0.597,0.597,0.597,0.597,0.693,0.679,0.679,0.679,0.679,0.718,0.718,0.718,0.614,0.614,0.584,0.679,0.584,0.679,0.679,0.679,0.584,0.584,0.584,0.713,0.713,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.655,0.655,0.655,0.584,0.58,0.58,0.58,0.532,0.58,0.614,0.584,0.584,0.614,0.614,0.614,0.614,0.532,0.532,0.532,0.532,0.583,0.583,0.583,0.583,0.609,0.609,0.609,0.609,0.609,0.585,0.585,0.585,0.585,0.585,0.585,0.585,0.585,0.573,0.573,0.573,0.573,0.573],"y":[24,21.6,34.7,33.4,36.2,28.7,22.9,27.1,16.5,18.9,15,18.9,21.7,20.4,18.2,19.9,23.1,17.5,20.2,18.2,13.6,19.6,15.2,14.5,15.6,13.9,16.6,14.8,18.4,21,12.7,14.5,13.2,13.1,13.5,18.9,20,21,24.7,30.8,34.9,26.6,25.3,24.7,21.2,19.3,20,16.6,14.4,19.4,19.7,20.5,25,23.4,18.9,35.4,24.7,31.6,23.3,19.6,18.7,16,22.2,25,33,23.5,19.4,22,17.4,20.9,24.2,21.7,22.8,23.4,24.1,21.4,20,20.8,21.2,20.3,28,23.9,24.8,22.9,23.9,26.6,22.5,22.2,23.6,28.7,22.6,22,22.9,25,20.6,28.4,21.4,38.7,43.8,33.2,27.5,26.5,18.6,19.3,20.1,19.5,19.5,20.4,19.8,19.4,21.7,22.8,18.8,18.7,18.5,18.3,21.2,19.2,20.4,19.3,22,20.3,20.5,17.3,18.8,21.4,15.7,16.2,18,14.3,19.2,19.6,23,18.4,15.6,18.1,17.4,17.1,13.3,17.8,14,14.4,13.4,15.6,11.8,13.8,15.6,14.6,17.8,15.4,21.5,19.6,15.3,19.4,17,15.6,13.1,41.3,24.3,23.3,27,50,50,50,22.7,25,50,23.8,23.8,22.3,17.4,19.1,23.1,23.6,22.6,29.4,23.2,24.6,29.9,37.2,39.8,36.2,37.9,32.5,26.4,29.6,50,32,29.8,34.9,37,30.5,36.4,31.1,29.1,50,33.3,30.3,34.6,34.9,32.9,24.1,42.3,48.5,50,22.6,24.4,22.5,24.4,20,21.7,19.3,22.4,28.1,23.7,25,23.3,28.7,21.5,23,26.7,21.7,27.5,30.1,44.8,50,37.6,31.6,46.7,31.5,24.3,31.7,41.7,48.3,29,24,25.1,31.5,23.7,23.3,22,20.1,22.2,23.7,17.6,18.5,24.3,20.5,24.5,26.2,24.4,24.8,29.6,42.8,21.9,20.9,44,50,36,30.1,33.8,43.1,48.8,31,36.5,22.8,30.7,50,43.5,20.7,21.1,25.2,24.4,35.2,32.4,32,33.2,33.1,29.1,35.1,45.4,35.4,46,50,32.2,22,20.1,23.2,22.3,24.8,28.5,37.3,27.9,23.9,21.7,28.6,27.1,20.3,22.5,29,24.8,22,26.4,33.1,36.1,28.4,33.4,28.2,22.8,20.3,16.1,22.1,19.4,21.6,23.8,16.2,17.8,19.8,23.1,21,23.8,23.1,20.4,18.5,25,24.6,23,22.2,19.3,22.6,19.8,17.1,19.4,22.2,20.7,21.1,19.5,18.5,20.6,19,18.7,32.7,16.5,23.9,31.2,17.5,17.2,23.1,24.5,26.6,22.9,24.1,18.6,30.1,18.2,20.6,17.8,21.7,22.7,22.6,25,19.9,20.8,16.8,21.9,27.5,21.9,23.1,50,50,50,50,50,13.8,13.8,15,13.9,13.3,13.1,10.2,10.4,10.9,11.3,12.3,8.8,7.2,10.5,7.4,10.2,11.5,15.1,23.2,9.7,13.8,12.7,13.1,12.5,8.5,5,6.3,5.6,7.2,12.1,8.3,8.5,5,11.9,27.9,17.2,27.5,15,17.2,17.9,16.3,7,7.2,7.5,10.4,8.8,8.4,16.7,14.2,20.8,13.4,11.7,8.3,10.2,10.9,11,9.5,14.5,14.1,16.1,14.3,11.7,13.4,9.6,8.7,8.4,12.8,10.5,17.1,18.4,15.4,10.8,11.8,14.9,12.6,14.1,13,13.4,15.2,16.1,17.8,14.9,14.1,12.7,13.5,14.9,20,16.4,17.7,19.5,20.2,21.4,19.9,19,19.1,19.1,20.1,19.9,19.6,23.2,29.8,13.8,13.3,16.7,12,14.6,21.4,23,23.7,25,21.8,20.6,21.2,19.1,20.6,15.2,7,8.1,13.6,20.1,21.8,24.5,23.1,19.7,18.3,21.2,17.5,16.8,22.4,20.6,23.9,22,11.9],"type":"scatter","mode":"markers","name":"medv vs nox","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(31,119,180,1)","line":{"color":"transparent"}},"xaxis":"x","yaxis":"y","frame":null},{"x":[0.385,0.389,0.392,0.392,0.394,0.398,0.398,0.4,0.4,0.4,0.4,0.401,0.401,0.401,0.403,0.403,0.403,0.404,0.404,0.404,0.405,0.405,0.405,0.409,0.409,0.409,0.41,0.41,0.41,0.411,0.411,0.411,0.411,0.411,0.411,0.413,0.413,0.413,0.413,0.413,0.413,0.415,0.415,0.4161,0.4161,0.4161,0.422,0.426,0.426,0.426,0.426,0.428,0.428,0.428,0.428,0.428,0.428,0.428,0.428,0.429,0.429,0.429,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.431,0.433,0.433,0.433,0.435,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.437,0.4379,0.4379,0.439,0.439,0.439,0.439,0.442,0.442,0.442,0.4429,0.4429,0.4429,0.4429,0.445,0.445,0.445,0.445,0.445,0.447,0.447,0.447,0.447,0.447,0.448,0.448,0.448,0.448,0.448,0.448,0.448,0.448,0.448,0.449,0.449,0.449,0.449,0.453,0.453,0.453,0.453,0.453,0.453,0.458,0.458,0.458,0.46,0.46,0.46,0.464,0.464,0.464,0.464,0.464,0.464,0.464,0.464,0.469,0.469,0.472,0.472,0.472,0.472,0.484,0.484,0.488,0.488,0.488,0.488,0.488,0.488,0.488,0.488,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.489,0.493,0.493,0.493,0.493,0.493,0.493,0.493,0.493,0.499,0.499,0.499,0.499,0.504,0.504,0.504,0.504,0.504,0.504,0.504,0.504,0.507,0.507,0.507,0.507,0.507,0.507,0.507,0.507,0.507,0.507,0.51,0.51,0.51,0.51,0.51,0.51,0.51,0.515,0.515,0.515,0.515,0.515,0.515,0.515,0.515,0.518,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.524,0.524,0.524,0.524,0.524,0.524,0.524,0.532,0.532,0.532,0.532,0.532,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.538,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.544,0.547,0.547,0.547,0.547,0.547,0.547,0.547,0.547,0.547,0.55,0.55,0.55,0.55,0.573,0.573,0.573,0.573,0.573,0.575,0.575,0.58,0.58,0.58,0.58,0.581,0.581,0.581,0.581,0.581,0.581,0.581,0.583,0.583,0.583,0.583,0.584,0.584,0.584,0.584,0.584,0.584,0.584,0.584,0.585,0.585,0.585,0.585,0.585,0.585,0.585,0.585,0.597,0.597,0.597,0.597,0.597,0.597,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.605,0.609,0.609,0.609,0.609,0.609,0.614,0.614,0.614,0.614,0.614,0.614,0.614,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.624,0.631,0.631,0.631,0.631,0.631,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.647,0.655,0.655,0.655,0.659,0.659,0.668,0.668,0.668,0.671,0.671,0.671,0.671,0.671,0.671,0.671,0.679,0.679,0.679,0.679,0.679,0.679,0.679,0.679,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.693,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.713,0.718,0.718,0.718,0.718,0.718,0.718,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.74,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871],"y":[28.2881932896387,28.1525290696041,28.0507809045781,28.0507809045781,27.9829487945608,27.8472845745261,27.8472845745261,27.7794524645088,27.7794524645088,27.7794524645088,27.7794524645088,27.7455364095001,27.7455364095001,27.7455364095001,27.6777042994828,27.6777042994828,27.6777042994828,27.6437882444742,27.6437882444742,27.6437882444742,27.6098721894655,27.6098721894655,27.6098721894655,27.4742079694309,27.4742079694309,27.4742079694309,27.4402919144222,27.4402919144222,27.4402919144222,27.4063758594135,27.4063758594135,27.4063758594135,27.4063758594135,27.4063758594135,27.4063758594135,27.3385437493962,27.3385437493962,27.3385437493962,27.3385437493962,27.3385437493962,27.3385437493962,27.2707116393789,27.2707116393789,27.2334039788694,27.2334039788694,27.2334039788694,27.0332992543183,26.8976350342836,26.8976350342836,26.8976350342836,26.8976350342836,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.8298029242663,26.7958868692576,26.7958868692576,26.7958868692576,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.7280547592403,26.660222649223,26.660222649223,26.660222649223,26.5923905392057,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.5245584291883,26.4940339796805,26.4940339796805,26.456726319171,26.456726319171,26.456726319171,26.456726319171,26.354978154145,26.354978154145,26.354978154145,26.3244537046372,26.3244537046372,26.3244537046372,26.3244537046372,26.2532299891191,26.2532299891191,26.253229989119,26.253229989119,26.2532299891191,26.1853978791017,26.1853978791017,26.1853978791017,26.1853978791017,26.1853978791017,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1514818240931,26.1175657690844,26.1175657690844,26.1175657690844,26.1175657690844,25.9819015490498,25.9819015490498,25.9819015490498,25.9819015490498,25.9819015490498,25.9819015490498,25.8123212740065,25.8123212740065,25.8123212740065,25.7444891639891,25.7444891639891,25.7444891639891,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,25.6088249439545,25.4392446689113,25.4392446689112,25.3374965038852,25.3374965038852,25.3374965038852,25.3374965038852,24.9305038437813,24.9305038437813,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.7948396237466,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.760923568738,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.6252593487033,24.4217630186514,24.4217630186514,24.4217630186514,24.4217630186514,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.252182743608,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.1504345785821,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,24.0486864135561,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.8791061385128,23.7773579734868,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.7095258634695,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.5738616434348,23.3025332033655,23.3025332033655,23.3025332033655,23.3025332033655,23.3025332033655,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,23.0990368733136,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.8955405432616,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.7937923782356,22.6920442132096,22.6920442132096,22.6920442132096,22.6920442132096,21.9119749480104,21.9119749480104,21.9119749480104,21.9119749480104,21.9119749480104,21.8441428379931,21.8441428379931,21.6745625629498,21.6745625629498,21.6745625629498,21.6745625629498,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.6406465079411,21.5728143979238,21.5728143979238,21.5728143979238,21.5728143979238,21.5388983429152,21.5388983429152,21.5388983429152,21.5388983429152,21.5388983429152,21.5388983429152,21.5388983429152,21.5388983429152,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.5049822879065,21.0979896278026,21.0979896278026,21.0979896278026,21.0979896278026,21.0979896278026,21.0979896278026,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.8266611877333,20.6909969676986,20.6909969676986,20.6909969676986,20.6909969676986,20.6909969676986,20.5214166926553,20.5214166926553,20.5214166926553,20.5214166926553,20.5214166926553,20.5214166926553,20.5214166926553,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,20.1822561425687,19.9448437575081,19.9448437575081,19.9448437575081,19.9448437575081,19.9448437575081,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.4021868773695,19.1308584373002,19.1308584373002,19.1308584373002,18.9951942172656,18.9951942172656,18.6899497221876,18.6899497221876,18.6899497221876,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.5882015571616,18.3168731170924,18.3168731170923,18.3168731170924,18.3168731170923,18.3168731170923,18.3168731170924,18.3168731170924,18.3168731170923,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.8420483469711,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.6046359619105,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,17.1637272467979,16.9941469717546,16.9941469717546,16.9941469717546,16.9941469717546,16.9941469717546,16.9941469717546,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,16.247993761564,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,15.2305121113042,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294,11.8049905554294],"type":"scatter","mode":"lines","name":"Best fit","line":{"fillcolor":"rgba(255,127,14,1)","color":"green"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<p><br></p>
<p>Therefore, even though the magnitudes are correct in the sense that comparing whole unit changes, <strong>nox</strong> would have a bigger impact than <strong>lstat</strong>, if we want to compare which of both actually has bigger influence, the best way to do this is by aligning the variable ranges. This can be done in a number of ways, for example, ensuring the range of the variables is always between 0 and 1. I am not going to cover the whole rescaling and interpretation of the models (you can do this yourself :)), but we have covered enough material to really understand the basics of Simple Linear Regression.</p>
<p>Our next step is to complicate things a little bit by creating models with model than one indepedent variable. These models are calling Multiple Linear Regression models.</p>
<p><br></p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">1.2</span> Multiple linear regression</h2>
<p><br></p>
<p>Most real-world analyses have more than one independent variable. Therefore, it is likely that you will be using multiple linear regression for most numeric prediction tasks. We can understand multiple regression as an extension of simple linear regression. The goal in both cases is similar-find values of beta coefficients that minimize the prediction error of a linear equation. The key difference is that there are additional terms for additional independent variables. Multiple regression equations generally follow the form of the following equation. The dependent variable y is specified as the sum of an intercept term <span class="math inline">\(\alpha\)</span> plus the product of the estimated <span class="math inline">\(\beta\)</span> value and the <span class="math inline">\(x\)</span> values for each of the <span class="math inline">\(i\)</span> features. An error term (denoted by the Greek letter epsilon) has been added here as a reminder that the predictions are not perfect. This represents the residual term.</p>
<p><span class="math display">\[ y = \alpha +  \beta_1 x_1 + \beta_2 x_2 + ... + \beta_i x_i + \varepsilon\]</span></p>
<p><br></p>
</div>
</div>
<div id="example---predicting-medical-expenses-using-linear-regression" class="section level1">
<h1><span class="header-section-number">2</span> Example - predicting medical expenses using linear regression</h1>
<p><br></p>
<p>In order for a health insurance company to make money, it needs to collect more in yearly premiums than it spends on medical care to its beneficiaries. As a result, insurers invest a great deal of time and money in developing models that accurately forecast medical expenses for the insured population.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">insurance =<span class="st"> </span><span class="kw">fread</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/insurance.txt&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)
insurance =<span class="st"> </span><span class="kw">as.data.frame</span>(insurance)
<span class="kw">names</span>(insurance)[<span class="kw">names</span>(insurance) ==<span class="st"> &#39;charges&#39;</span>] =<span class="st"> &#39;expenses&#39;</span>

<span class="kw">head</span>(insurance)</code></pre></div>
<pre><code>##   age    sex    bmi children smoker    region  expenses
## 1  19 female 27.900        0    yes southwest 16884.924
## 2  18   male 33.770        1     no southeast  1725.552
## 3  28   male 33.000        3     no southeast  4449.462
## 4  33   male 22.705        0     no northwest 21984.471
## 5  32   male 28.880        0     no northwest  3866.855
## 6  31 female 25.740        0     no southeast  3756.622</code></pre>
<p><br></p>
<p>The insurance.csv file includes 1,338 examples of beneficiaries currently enrolled in the insurance plan, with features indicating characteristics of the patient as well as the total medical expenses charged to the plan for the calendar year. The features are:</p>
<ul>
<li><strong>age:</strong> An integer indicating the age of the primary beneficiary (excluding those above 64 years, since they are generally covered by the government).</li>
<li><strong>sex:</strong> The policy holder’s gender, either male or female.</li>
<li><strong>bmi:</strong> The body mass index (BMI), which provides a sense of how over- or under-weight a person is relative to their height. BMI is equal to weight (in kilograms) divided by height (in meters) squared. An ideal BMI is within the range of 18.5 to 24.9.</li>
<li><strong>children:</strong> An integer indicating the number of children/dependents covered by the insurance plan.</li>
<li><strong>smoker:</strong> A yes or no categorical variable that indicates whether the insured regularly smokes tobacco.</li>
<li><strong>region:</strong> The beneficiary’s place of residence in the US, divided into four geographic regions: northeast, southeast, southwest, or northwest.</li>
</ul>
<p>Our model’s dependent variable is expenses, which measures the medical costs each person charged to the insurance plan for the year. Prior to building a regression model, it is often helpful to check for normality. Although linear regression does not strictly require a normally distributed dependent variable, the model often fits better when this is true. Let’s take a look at the summary statistics.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(insurance$expenses)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1122    4740    9382   13270   16640   63770</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(insurance$expenses)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" /><!-- --></p>
<p><br></p>
<p>As expected, the figure shows a right-skewed distribution. It also shows that the majority of people in our data have yearly medical expenses between zero and $15,000, in spite of the fact that the tail of the distribution extends far past these peaks. Although this distribution is not ideal for a linear regression, knowing this weakness ahead of time may help us design a better-fitting model later on.</p>
<p>Before we address that issue, another problem is at hand. <strong>Regression models require that every feature is numeric</strong>, yet we have three factor-type features in our data frame. For instance, the sex variable is divided into male and female levels, while smoker is divided into yes and no. From the summary() output, we know that the region variable has four levels.</p>
<p><br></p>
<div id="exploring-relationships-among-features---the-correlation-matrix" class="section level2">
<h2><span class="header-section-number">2.1</span> Exploring relationships among features - the correlation matrix</h2>
<p><br></p>
<p>Before fitting a regression model to data, it can be useful to determine how the independent variables are related to the dependent variable and each other. A correlation matrix provides a quick overview of these relationships. Given a set of variables, it provides a correlation for each pairwise relationship. To create a correlation matrix for the four numeric variables in the insurance data frame, use the cor() command:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(insurance[<span class="kw">c</span>(<span class="st">&quot;age&quot;</span>, <span class="st">&quot;bmi&quot;</span>, <span class="st">&quot;children&quot;</span>, <span class="st">&quot;expenses&quot;</span>)])</code></pre></div>
<pre><code>##                age       bmi   children   expenses
## age      1.0000000 0.1092719 0.04246900 0.29900819
## bmi      0.1092719 1.0000000 0.01275890 0.19834097
## children 0.0424690 0.0127589 1.00000000 0.06799823
## expenses 0.2990082 0.1983410 0.06799823 1.00000000</code></pre>
<p><br></p>
<p>At the intersection of each row and column pair, the correlation is listed for the variables indicated by that row and column. The diagonal is always 1 since there is always a perfect correlation between a variable and itself. The values above and below the diagonal are identical since correlations are symmetrical. In other words, cor(x, y) is equal to cor(y, x). None of the correlations in the matrix are considered strong, but there are some notable associations. For instance, age and bmi appear to have a weak positive correlation, meaning that as someone ages, their body mass tends to increase. There is also a moderate positive correlation between age and expenses, bmi and expenses, and children and expenses. These associations imply that as age, body mass, and number of children increase, the expected cost of insurance goes up.</p>
<p><br></p>
</div>
<div id="visualizing-relationships-among-features---the-scatterplot-matrix" class="section level2">
<h2><span class="header-section-number">2.2</span> Visualizing relationships among features - the scatterplot matrix</h2>
<p><br></p>
<p>It can also be helpful to visualize the relationships among numeric features by using a scatterplot. Although we could create a scatterplot for each possible relationship, doing so for a large number of features might become tedious. An alternative is to create a scatterplot matrix (sometimes abbreviated as SPLOM), which is simply a collection of scatterplots arranged in a grid. It is used to detect patterns among three or more variables. The scatterplot matrix is not a true multidimensional visualization because only two features are examined at a time. Still, it provides a general sense of how the data may be interrelated.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairs.panels</span>(insurance[<span class="kw">c</span>(<span class="st">&quot;age&quot;</span>, <span class="st">&quot;bmi&quot;</span>, <span class="st">&quot;children&quot;</span>, <span class="st">&quot;expenses&quot;</span>)])</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-14-1.png" /><!-- --></p>
<p><br></p>
<p>In the scatterplot matrix, the intersection of each row and column holds the scatterplot of the variables indicated by the row and column pair. The diagrams above and below the diagonal are transpositions since the x axis and y axis have been swapped. Do you notice any patterns in these plots? Although some look like random clouds of points, a few seem to display some trends. The relationship between age and expenses displays several relatively straight lines, while the bmi versus expenses plot has two distinct groups of points. It is difficult to detect trends in any of the other plots.</p>
<p>Above the diagonal, the scatterplots have been replaced with a correlation matrix. On the diagonal, a histogram depicting the distribution of values for each feature is shown. Finally, the scatterplots below the diagonal are now presented with additional visual information.</p>
<p>The oval-shaped object on each scatterplot is a correlation ellipse. It provides a visualization of correlation strength. The dot at the center of the ellipse indicates the point at the mean values for the x and y axis variables. The correlation between the two variables is indicated by the shape of the ellipse; the more it is stretched, the stronger the correlation. An almost perfectly round oval, as with bmi and children, indicates a very weak correlation (in this case, it is 0.01).</p>
<p>The curve drawn on the scatterplot is called a loess curve. It indicates the general relationship between the x and y axis variables. It is best understood by example. The curve for age and children is an upside-down U, peaking around middle age. This means that the oldest and youngest people in the sample have fewer children on the insurance plan than those around middle age. Because this trend is non-linear, this finding could not have been inferred from the correlations alone. On the other hand, the loess curve for age and bmi is a line sloping gradually up, implying that body mass increases with age, but we had already inferred this from the correlation matrix.</p>
<p><br></p>
</div>
<div id="training-a-model-on-the-data" class="section level2">
<h2><span class="header-section-number">2.3</span> Training a model on the data</h2>
<p><br></p>
<p>The following command fits a linear regression model relating the six independent variables to the total medical expenses.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ins_model =<span class="st"> </span><span class="kw">lm</span>(expenses ~<span class="st"> </span>., <span class="dt">data =</span> insurance)
<span class="kw">summary</span>(ins_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = expenses ~ ., data = insurance)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11304.9  -2848.1   -982.1   1393.9  29992.8 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -11938.5      987.8 -12.086  &lt; 2e-16 ***
## age                256.9       11.9  21.587  &lt; 2e-16 ***
## sexmale           -131.3      332.9  -0.394 0.693348    
## bmi                339.2       28.6  11.860  &lt; 2e-16 ***
## children           475.5      137.8   3.451 0.000577 ***
## smokeryes        23848.5      413.1  57.723  &lt; 2e-16 ***
## regionnorthwest   -353.0      476.3  -0.741 0.458769    
## regionsoutheast  -1035.0      478.7  -2.162 0.030782 *  
## regionsouthwest   -960.0      477.9  -2.009 0.044765 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6062 on 1329 degrees of freedom
## Multiple R-squared:  0.7509, Adjusted R-squared:  0.7494 
## F-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><br></p>
<p>Understanding the regression coefficients is fairly straightforward. The intercept is the predicted value of expenses when the independent variables are equal to zero. As is the case here, quite often the intercept is of little value alone because it is impossible to have values of zero for all features. For example, since no person exists with age zero and BMI zero, the intercept has no real-world interpretation. For this reason, in practice, the intercept is often ignored.</p>
<p>The coefficients indicate the estimated increase in expenses for an increase of one in each of the features, assuming all other values are held constant. For instance, for each additional year of age, we would expect 256.90 higher medical expenses on average, assuming everything else is equal. Similarly, each additional child results in an average of 475.50 in additional medical expenses each year, and each unit increase in BMI is associated with an average increase of 339.30 in yearly medical expenses, all else equal.</p>
<p>You might notice that although we only specified six features in our model formula, there are eight coefficients reported in addition to the intercept. This happened because the lm() function automatically applied a technique known as dummy coding to each of the factor-type variables we included in the model. Dummy coding allows a nominal feature to be treated as numeric by creating a binary variable, often called a dummy variable, for each category of the feature. The dummy variable is set to 1 if the observation falls into the specified category or 0 otherwise. For instance, the sex feature has two categories: male and female. This will be split into two binary variables, which R names sexmale and sexfemale. For observations where sex = male, then sexmale = 1 and sexfemale = 0; conversely, if sex = female, then sexmale = 0 and sexfemale = 1. The same coding applies to variables with three or more categories. For example, R split the four-category feature region into four dummy variables: regionnorthwest, regionsoutheast, regionsouthwest, and regionnortheast.</p>
<p>When adding a dummy variable to a regression model, one category is always left out to serve as the reference category. The estimates are then interpreted relative to the reference. In our model, R automatically held out the sexfemale, smokerno, and regionnortheast variables, making female non-smokers in the northeast region the reference group. Thus, males have 131.40 less medical expenses each year relative to females and smokers cost an average of 23,847.50 more than non-smokers per year. The coefficient for each of the three regions in the model is negative, which implies that the reference group, the northeast region, tends to have the highest average expenses.</p>
<p>The results of the linear regression model make logical sense: old age, smoking, and obesity tend to be linked to additional health issues, while additional family member dependents may result in an increase in physician visits and preventive care such as vaccinations and yearly physical exams. However, we currently have no sense of how well the model is fitting the data.</p>
<p><br></p>
</div>
<div id="evaluating-model-performance" class="section level2">
<h2><span class="header-section-number">2.4</span> Evaluating model performance</h2>
<p><br></p>
<p>The summary() output may seem confusing at first, but the basics are easy to pick up. As indicated by the numbered labels in the preceding output, the output provides three key ways to evaluate the performance, or fit, of our model:</p>
<ol style="list-style-type: decimal">
<li><p>The residuals section provides summary statistics for the errors in our predictions, some of which are apparently quite substantial. Since a residual is equal to the true value minus the predicted value, the maximum error of 29981.7 suggests that the model under-predicted expenses by nearly 30,000 for at least one observation. On the other hand, 50 percent of errors fall within the 1Q and 3Q values (the first and third quartile), so the majority of predictions were between 2,850.90 over the true value and 1,383.90 under the true value.</p></li>
<li><p>For each estimated regression coefficient, the p-value, denoted Pr(&gt;|t|), provides an estimate of the probability that the true coefficient is zero given the value of the estimate. Small p-values suggest that the true coefficient is very unlikely to be zero, which means that the feature is extremely unlikely to have no relationship with the dependent variable. Note that some of the p-values have stars (***), which correspond to the footnotes to indicate the significance level met by the estimate. This level is a threshold, chosen prior to building the model, which will be used to indicate “real” findings, as opposed to those due to chance alone; p-values less than the significance level are considered statistically significant. If the model had few such terms, it may be cause for concern, since this would indicate that the features used are not very predictive of the outcome. Here, our model has several highly significant variables, and they seem to be related to the outcome in logical ways.</p></li>
<li><p>The multiple R-squared value (also called the coefficient of determination) provides a measure of how well our model as a whole explains the values of the dependent variable. It is similar to the correlation coefficient, in that the closer the value is to 1.0, the better the model perfectly explains the data. Since the R-squared value is 0.7494, we know that the model explains nearly 75 percent of the variation in the dependent variable. Because models with more features always explain more variation, the adjusted R-squared value corrects R-squared by penalizing models with a large number of independent variables. It is useful for comparing the performance of models with different numbers of explanatory variables.</p></li>
</ol>
<p><br></p>
</div>
<div id="improving-model-performance" class="section level2">
<h2><span class="header-section-number">2.5</span> Improving model performance</h2>
<p><br></p>
<p>A key difference between the regression modeling and other machine learning approaches is that regression typically leaves feature selection and model specification to the user. Consequently, if we have subject matter knowledge about how a feature is related to the outcome, we can use this information to inform the model specification and potentially improve the model’s performance.</p>
<p><br></p>
<div id="adding-non-linear-relationships" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Adding non-linear relationships</h3>
<p><br></p>
<p>In linear regression, the relationship between an independent variable and the dependent variable is assumed to be linear, yet this may not necessarily be true. For example, the effect of age on medical expenditure may not be constant throughout all the age values; the treatment may become disproportionately expensive for oldest populations. To account for a non-linear relationship, we can add a higher order term to the regression model, treating the model as a polynomial.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">insurance$age2 =<span class="st"> </span>insurance$age^<span class="dv">2</span></code></pre></div>
<p><br></p>
</div>
<div id="converting-a-numeric-variable-to-a-binary-indicator" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Converting a numeric variable to a binary indicator</h3>
<p><br></p>
<p>Suppose we have a hunch that the effect of a feature is not cumulative, rather it has an effect only after a specific threshold has been reached. For instance, BMI may have zero impact on medical expenditures for individuals in the normal weight range, but it may be strongly related to higher costs for the obese (that is, BMI of 30 or above). We can model this relationship by creating a binary obesity indicator variable that is 1 if the BMI is at least 30, and 0 if less. The estimated beta for this binary feature would then indicate the average net impact on medical expenses for individuals with BMI of 30 or above, relative to those with BMI less than 30.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">insurance$bmi30 =<span class="st"> </span><span class="kw">ifelse</span>(insurance$bmi &gt;=<span class="st"> </span><span class="dv">30</span>, <span class="dv">1</span>, <span class="dv">0</span>)</code></pre></div>
<p><br></p>
</div>
<div id="adding-interaction-effects" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Adding interaction effects</h3>
<p><br></p>
<p>What if certain features have a combined impact on the dependent variable? For instance, smoking and obesity may have harmful effects separately, but it is reasonable to assume that their combined effect may be worse than the sum of each one alone. When two features have a combined effect, this is known as an interaction. To have the obesity indicator (bmi30) and the smoking indicator (smoker) interact, we would write a formula in the form expenses ~ bmi30*smoker:</p>
<p><br></p>
</div>
<div id="putting-it-all-together---an-improved-regression-model" class="section level3">
<h3><span class="header-section-number">2.5.4</span> Putting it all together - an improved regression model</h3>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ins_model2 =<span class="st"> </span><span class="kw">lm</span>(expenses ~<span class="st"> </span>age +<span class="st"> </span>age2 +<span class="st"> </span>children +<span class="st"> </span>bmi +<span class="st"> </span>sex +<span class="st"> </span>bmi30*smoker +<span class="st"> </span>region, <span class="dt">data =</span> insurance)
<span class="kw">summary</span>(ins_model2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = expenses ~ age + age2 + children + bmi + sex + bmi30 * 
##     smoker + region, data = insurance)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17296.4  -1656.0  -1263.3   -722.1  24160.2 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       134.2509  1362.7511   0.099 0.921539    
## age               -32.6851    59.8242  -0.546 0.584915    
## age2                3.7316     0.7463   5.000 6.50e-07 ***
## children          678.5612   105.8831   6.409 2.04e-10 ***
## bmi               120.0196    34.2660   3.503 0.000476 ***
## sexmale          -496.8245   244.3659  -2.033 0.042240 *  
## bmi30           -1000.1403   422.8402  -2.365 0.018159 *  
## smokeryes       13404.6866   439.9491  30.469  &lt; 2e-16 ***
## regionnorthwest  -279.2038   349.2746  -0.799 0.424212    
## regionsoutheast  -828.5467   351.6352  -2.356 0.018604 *  
## regionsouthwest -1222.6437   350.5285  -3.488 0.000503 ***
## bmi30:smokeryes 19810.7533   604.6567  32.764  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4445 on 1326 degrees of freedom
## Multiple R-squared:  0.8664, Adjusted R-squared:  0.8653 
## F-statistic: 781.7 on 11 and 1326 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><br></p>
<p>So, how has the new model performed compared to the first one we built?</p>
<ul>
<li>Residuals. For the first model the maximum error was 30k, whereas in the second one is 24k. Not the best improvement, but for the extreme cases we have been able to improve large errors. In addition, for the first model, 50% of the cases (cases between Q1 and Q3) fell between -2848.1 and 1393.9, whereas in the second model between -1656.0 and -722.1. Now this is a very good sign that our model is doing better.</li>
<li>R-squared. The first model had an r-squared of 0.7494, whereas the new model has 0.8653. Again a good indicator of better performance for linear regression.</li>
<li>Non signficant features. You can observe how in the second model, the newly defined features are all significant. As an example, we previously were checking age linearly and it was a significant variable in the first model. However, by adding a non-linear relationship of age with expenses, we have a better predictive variable which is significant, leaving the linear age-expenses relationship non significant.</li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="machine-learning-theory" class="section level1">
<h1><span class="header-section-number">3</span> Machine Learning Theory</h1>
<p><br></p>
<p>Up until now we have understood linear regression on a high level: a little bit of the construction of the formula, how to implement a linear regression model in R, checking initial results from a model and adding extra terms to help with our modelling (non-linear relationships, interaction terms and dummy/flag variables). All of this is important, and the example of medical expenses gives you a good walkthrough to understand all of this.</p>
<ul>
<li><p>However, what we haven’t yet done is created a model that can generalise future data. If you check the medical expenses example, we have built our model using the whole dataset, data that we already know the result of, but we haven’t had the chance to check how would it perform on unseen data! And that is our real primary goal, how to create a model that will generalise well and that we trust when using it predict with future input data.</p></li>
<li><p>In addition, we havent explored other possible metrics for linear regression. R-squared is a high level metric, but that we need to be careful with (I will show later on why). An interesting feature I always like to check is the root mean squared error. We haven’t either plotted any of our predictions against the actual data, or checked what do the residuals of our model look like.</p></li>
<li><p>Finally, linear regression can have other modelling inputs that we haven’t yet introduced (like regularization parameters). We will explore these and see how they are implemented in R.</p></li>
</ul>
<p>These following sections are going to be a bit theoretical, with little or no R scripting examples. I believe that understanding the theory first and then showcasing with examples is the best way to understand things. You can always jump directly to the examples at the end if you want to skip the theory.</p>
<p><br></p>
<div id="machine-learning-theory---univariate-linear-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> Machine Learning Theory - Univariate Linear Regression</h2>
<p><br></p>
<p>Earlier in this post we have seen the basic construction of a linear regression function:</p>
<p><span class="math display">\[ y = \alpha +  \beta_1 x_1 + \beta_2 x_2 + ... + \beta_i x_i + \varepsilon\]</span></p>
<p>We also answered the question of “how do we choose the <span class="math inline">\(\beta\)</span> coefficients”. We said the approach was to take the OLS approach, which basically minimises the errors between actual and predicted values. This, in essence, is totally correct. However, what we are going to introduce here is the formal explanation of how the algorithm performs these checks.</p>
<p><br></p>
<div id="hypothesis-for-univariate-linear-regression" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Hypothesis for univariate linear regression</h3>
<p><br></p>
<p><strong>Hypothesis.</strong> The hypothesis formula should seem familiar to you as it represents a linear equation. In this case, our output (our hypothesis) is dependant of the value of x. The theta values are the ones we want to choose to fit the data as perfectly as possible.</p>
<p><span class="math display">\[ h_{\theta(x)} = \theta_0 + \theta_1x \]</span></p>
<p><strong>Goal.</strong> Minimize the squared error difference between our predicted hypothesis and the actual value of y. This total errors can be seen as a cost, eg, the bigger the total errors, the bigger cost you would have, and hence, the idea is to minimize a cost function.</p>
<p><span class="math display">\[ \min_{\theta_0,\theta_1} J(\theta_0,\theta_1) = \min_{\theta_0,\theta_1} \frac{1}{2m} \sum_{i=1}^m \big( h_{\theta} \big(x^{(i)} \big)-y^{(i)} \big)^2 \]</span></p>
<p>being:</p>
<ul>
<li><span class="math inline">\(J(\theta_0,\theta_1)\)</span>, the cost function</li>
<li><span class="math inline">\(m\)</span>, the total number of training examples</li>
<li><span class="math inline">\(x(i)\)</span>, i-th training example</li>
<li><span class="math inline">\(y(i)\)</span>, i-th actual value</li>
</ul>
<p><br></p>
</div>
<div id="cost-function-for-univariate-linear-regression---intuition" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Cost function for univariate linear regression - intuition</h3>
<p><br></p>
<p>Ok, this cost function might seems a bit scary for those of you who do not feel comfortable with maths. I will try to use a numeric example to help you understand this. Let’s simplify the example and eliminate the independent term <span class="math inline">\(\theta_0\)</span>:</p>
<p><img src="images/3_linRegTable1.png" width="777" /></p>
<p>The following table shows in initial data (blue colour) and the different hypothesis outputs we get when changing <span class="math inline">\(\theta_1\)</span>:</p>
<p><img src="images/4_linRegTable2.png" width="1012" /><img src="images/5_linRegGraph1.png" width="772" /></p>
<p>It is quite obvious that <span class="math inline">\(\theta_1 = 1\)</span> (white columns) would be the best choice as it fits perfectly the real data! All other possibilities (purple, green, red lines) are different from the real data, and therefore will have an error associated with it. So, how does these different possible <span class="math inline">\(\theta_1\)</span> values affect the cost function <span class="math inline">\(J\)</span>?</p>
<p>We calculate the squared difference for each example y and hypothesis h. Then we add all error as the sum of sq. errors, and finally we apply the cost function equation to get <span class="math inline">\(J\)</span>:</p>
<p><img src="images/6_linRegTable3.png" width="1233" /><img src="images/7_linRegGraph2.png" width="762" /></p>
<p><br></p>
</div>
<div id="gradient-descent-algorithm" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Gradient descent algorithm</h3>
<p><br></p>
<p>Ok, we as humans are able to say with a single glance that minimum point for the cost function would correspond to a combination of <span class="math inline">\(\theta_0 = 0\)</span>, <span class="math inline">\(\theta_1 = 1\)</span>. But, how does the machine get to this point? What steps does it have to follow in order to choose from infinite combinations, which is the best pair, the one that minimizes the cost function? To do this, we introduce Gradient Descent. Gradient descent is an iteration process that updates the values of the <span class="math inline">\(\theta\)</span> parameters until we reach convergence, in other words, it is a process that keeps updating <span class="math inline">\(\theta\)</span> until there is no difference in the last repetition (because we have reached the minimum and there is no better solution to update with a different value).</p>
<p><img src="images/8_linRegTable4.png" width="770" /></p>
<p>Again, this formulation might be a bit overwhelming for those not comfortable with maths, especially as we introduce partial derivatives. In addition, we have introduced a new parameter ?? that we haven’t yet defined. So, let’s try to explain how gradient descent works with a graphic example.</p>
<ul>
<li>Partial derivatives. It graphically represents the slope of the curve on a certain point. The important information we need from the slope is if it’s positive or negative. In this case, we have a positive slope, therefore, we have a positive term for the partial derivatives in the equation. Taking into consideration that alpha is positive, then applying gradient descent, the next step will update theta with a smaller value, and therefore, getting closer to the minimum!</li>
</ul>
<p><img src="images/9_linRegGraph3.png" width="611" /></p>
<ul>
<li>Effect of alpha. Alpha is a positive constant and there are 2 main extreme effects that could jeopardize the implementation of gradient descent:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Alpha too small: a minimum will be found, but it will take too many operations to find it. Slow but accurate.</li>
<li>Alpha too big: as alpha is too big, the steps taken might be too big, and therefore, we might be jumping over the minimum without reaching it. Quicker but we might be able to converge.</li>
</ol>
<p><img src="images/10_linRegGraph4.png" width="613" /><img src="images/11_linRegGraph5.png" width="614" /></p>
<p><br></p>
</div>
<div id="gradient-descent-algorithm-for-linear-regression" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Gradient descent algorithm for linear regression</h3>
<p><br></p>
<p>So once understood how gradient descent works, let me formulate the equation and apply it for the case of 1 variable using linear regression.</p>
<p><br></p>
<p><img src="images/12_linRegTable5.png" width="770" /></p>
<p><br></p>
</div>
</div>
<div id="machine-learning-theory---multivariate-linear-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Machine Learning Theory - Multivariate Linear Regression</h2>
<p><br></p>
<p>Let’s introduce a simple example to show what we mean by having a linear model with multiple independent variables.</p>
<p><br></p>
<p><img src="images/13_multivariateTable1.png" width="764" /></p>
<p><br></p>
<p>We want to predict house prices (variable <span class="math inline">\(y\)</span>), and we have 3 different features (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span>). How would our hypothesis function look like? And what would be the general formula?</p>
<p><span class="math display">\[ h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_1x_1 \]</span></p>
<p>This formula looks fine on paper, but if you want to implement it in code, the best idea would be to vectorise it (throughout all theory posts, the final goal will be always to vectorise formulas).</p>
<p><span class="math display">\[ X = 
\begin{bmatrix} 
x_0 \\
x_1 \\
... \\
x_n
\end{bmatrix},
theta = 
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
... \\
\theta_n
\end{bmatrix}
\longrightarrow
h_\theta(x) = \theta^T X\]</span></p>
<p><br></p>
<div id="gradient-descent-for-multiple-features" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Gradient descent for multiple features</h3>
<p><br></p>
<p>Recall the formulas used for univariate linear regression:</p>
<p><br></p>
<p><img src="images/14_multivariateEq3.png" width="648" /></p>
<p><br></p>
<p>The only change that will differentiate gradient descent for multiple features will be the generalization of the formula for univariate linear regression gradient descent. In fact, the previous formula will be replace for the generalised one because it covers both the univariate and multivariate situation.</p>
<p><br></p>
<p><img src="images/15_multivariateEq4.png" width="376" /><img src="images/16_multivariateEq5.png" width="674" /></p>
<p><br></p>
</div>
<div id="changes-to-gradient-descent-feature-scaling" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Changes to gradient descent: feature scaling</h3>
<p><br></p>
<p>There are 2 main ideas behind feature scaling: * For intuition purposes it is easier to compare 2 features that are normalised. For example how to compare: 1. X1: size (range from 10-2000 m2) 2. X2: number beds (range from 1-10) - They are 2 variables with very different ranges and this is difficult to compare directly with a single number. * For running / debugging purposes, feature scaling speeds up the algorithm and tends to find the minimum quicker.</p>
<p><br></p>
<p><img src="images/17_multivariateTable2.png" width="769" /><img src="images/18_multivariateGraph1.png" width="378" /></p>
<p><br></p>
</div>
<div id="changes-to-gradient-descent-learning-rate" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Changes to gradient descent: learning rate</h3>
<p><br></p>
<p>Recall we saw what happened with different learning rates for univariate linear regression (it could be really slow or it could even diverge and not reach a minimum). In this section we will make sure that gradient descent is working correctly with a debugging method. We understand that, with every iteration of gradient descent, there is an update of the <span class="math inline">\(\theta\)</span> values that reflect in smaller magnitudes of the cost function, which imply we are moving towards a minimum. With that premise, the idea is to plot the cost function J against the number of iterations we are using in the gradient descent loop.</p>
<p><br></p>
<p><img src="images/19_multivariateGraph2.png" width="585" /><img src="images/20_multivariateGraph3.png" width="592" /></p>
<p><br></p>
</div>
<div id="computing-parameters-analytically-normal-equation" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Computing parameters analytically: normal equation</h3>
<p><br></p>
<p>In this section, we will introduce another method to solve the THETA values. Until now, we have seen how to calculate a linear model to adjust the data using and remember, that do this, we used gradient descent which is an iterative process. As we saw, 2 of the main problems gradient descent has are: 1.The need of feature scaling, 2.The need of choosing alpha. These can be supressed with the introduction of the normal equation. To explain it lets introduce a numeric example:</p>
<p><br></p>
<p><img src="images/21_multivariateTable3.png" width="772" /></p>
<p><br></p>
<p>We can solve this problem by vectorising it!</p>
<p><br></p>
<p><img src="images/22_multivariateEq6.png" width="688" /></p>
<p><br></p>
<p>I am not going to demonstrate the steps in order to get the final theta formula, but it can de shown that using a vectorised approach, we can analytically calculate theta parameters using:</p>
<p><br></p>
<p><img src="images/23_multivariateEq7.png" width="219" /></p>
<p><br></p>
<p>It seems that using the normal equation is much simpler than using gradient descent right? Well, as always, there are caveats when using it. If you haven’t yet guessed, its main problem is when dealing with a lot of features. Say for example we have more than 10,000 features:</p>
<p><br></p>
<p><img src="images/24_multivariateEq8.png" width="744" /><img src="images/25_multivariateTable4.png" width="776" /></p>
<p><br></p>
</div>
</div>
</div>
<div id="example-using-a-proper-modelling-approach---hyundai-elantra-car-sales" class="section level1">
<h1><span class="header-section-number">4</span> Example using a proper modelling approach - Hyundai Elantra Car Sales</h1>
<p><br></p>
<div id="goal" class="section level2">
<h2><span class="header-section-number">4.1</span> Goal</h2>
<p><br></p>
<p>Consider a company that produces and sells a product. In a given period, if the company produces more units than how many consumers will buy, the company will not earn money on the unsold units and will incur additional costs due to having to store those units in inventory before they can be sold. If it produces fewer units than how many consumers will buy, the company will earn less than it potentially could have earned. Being able to predict consumer sales, therefore, is of first order importance to the company. In this problem, we will try to predict monthly sales of the Hyundai Elantra in the United States.</p>
<p><br></p>
</div>
<div id="dataset" class="section level2">
<h2><span class="header-section-number">4.2</span> Dataset</h2>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">elantra =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;elantra.csv&quot;</span>)
<span class="kw">str</span>(elantra)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    50 obs. of  7 variables:
##  $ Month       : int  1 1 1 1 1 2 2 2 2 2 ...
##  $ Year        : int  2010 2011 2012 2013 2014 2010 2011 2012 2013 2014 ...
##  $ ElantraSales: int  7690 9659 10900 12174 15326 7966 12289 13820 16219 16393 ...
##  $ Unemployment: num  9.7 9.1 8.2 7.9 6.6 9.8 9 8.3 7.7 6.7 ...
##  $ Queries     : int  153 259 354 230 232 130 266 296 239 240 ...
##  $ CPI_energy  : num  213 229 244 243 248 ...
##  $ CPI_all     : num  217 221 228 231 235 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(elantra)</code></pre></div>
<pre><code>##   Month Year ElantraSales Unemployment Queries CPI_energy CPI_all
## 1     1 2010         7690          9.7     153    213.377 217.466
## 2     1 2011         9659          9.1     259    229.353 221.082
## 3     1 2012        10900          8.2     354    244.178 227.666
## 4     1 2013        12174          7.9     230    242.560 231.321
## 5     1 2014        15326          6.6     232    247.575 234.933
## 6     2 2010         7966          9.8     130    209.924 217.251</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(elantra)</code></pre></div>
<pre><code>##      Month           Year       ElantraSales    Unemployment      Queries        CPI_energy       CPI_all     
##  Min.   : 1.0   Min.   :2010   Min.   : 7690   Min.   :6.600   Min.   :130.0   Min.   :204.2   Min.   :217.3  
##  1st Qu.: 3.0   1st Qu.:2011   1st Qu.:12560   1st Qu.:7.725   1st Qu.:224.8   1st Qu.:230.1   1st Qu.:221.3  
##  Median : 6.0   Median :2012   Median :15624   Median :8.250   Median :262.5   Median :244.4   Median :227.9  
##  Mean   : 6.3   Mean   :2012   Mean   :16005   Mean   :8.422   Mean   :263.5   Mean   :236.9   Mean   :226.7  
##  3rd Qu.: 9.0   3rd Qu.:2013   3rd Qu.:19197   3rd Qu.:9.100   3rd Qu.:311.0   3rd Qu.:247.1   3rd Qu.:231.7  
##  Max.   :12.0   Max.   :2014   Max.   :26153   Max.   :9.900   Max.   :427.0   Max.   :256.4   Max.   :235.2</code></pre>
<ol style="list-style-type: decimal">
<li>Month = the month of the year for the observation (1 = January, 2 = February, 3 = March, …).</li>
<li>Year = the year of the observation.</li>
<li>ElantraSales = the number of units of the Hyundai Elantra sold in the United States in the given month.</li>
<li>Unemployment = the estimated unemployment percentage in the United States in the given month.</li>
<li>Queries = a (normalized) approximation of the number of Google searches for “hyundai elantra” in the given month.</li>
<li>CPI_energy = the monthly consumer price index (CPI) for energy for the given month.</li>
<li>CPI_all = the consumer price index (CPI) for all products for the given month; this is a measure of the magnitude of the prices paid by consumer households for goods and services (e.g., food, clothing, electricity, etc.).</li>
</ol>
<p>From the summary we don’t see any missing values and no strange extreme values for the variables. Let’s assume this, and begin with the modelling steps. The only thing to consider is that this dataset contains only 50 records! We will use this first example as a way to show the methodology of creating a linear regression model, but we wouldn’t trust a model with so few records!</p>
<p><br></p>
</div>
<div id="extra-understanding" class="section level2">
<h2><span class="header-section-number">4.3</span> Extra understanding</h2>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairs.panels</span>(elantra)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-36-1.png" /><!-- --></p>
<p>By checking the facet above, we can observe certain interesting points:</p>
<ul>
<li>The correlations of Sales with all variables expect month seem to be quite strong (correlation values and stretched ellipse).</li>
<li>It seems like you could fit a linear relationship between Sales and Unemployment.</li>
<li>Might be interesting the test later what happens with non-linear relationships between Sales and CPI_energy and CPI_all</li>
</ul>
<p><br></p>
</div>
<div id="splitting-the-data" class="section level2">
<h2><span class="header-section-number">4.4</span> Splitting the data</h2>
<p><br></p>
<p>There are multiple ways to split the data. One way is to randomnly select samples of data and create a train / test split. In this case we can take another approach which is split depending on time. For example, if a stakeholder asks us, <em>“how can I trust this model is going to be predictive next year”</em>, we could give a simple answer of <em>“we trained the data with 2012 data, and checked how good it was at checking data in 2013. Given a successful model, we can take the same methodology and use 2013 to predict 2014.”</em>. Let’s take this approach and split the data with records before and after 2012.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># TRAINING AND TESTING SET</span>
ElantraTrain =<span class="st"> </span><span class="kw">subset</span>(elantra, Year &lt;=<span class="st"> </span><span class="dv">2012</span>)
ElantraTest =<span class="st"> </span><span class="kw">subset</span>(elantra, Year &gt;<span class="st"> </span><span class="dv">2012</span>)

<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Number rows in train data:&quot;</span>,<span class="kw">nrow</span>(ElantraTrain)))</code></pre></div>
<pre><code>## [1] &quot;Number rows in train data: 36&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Number rows in test data:&quot;</span>,<span class="kw">nrow</span>(ElantraTest)))</code></pre></div>
<pre><code>## [1] &quot;Number rows in test data: 14&quot;</code></pre>
<p><br></p>
</div>
<div id="building-models" class="section level2">
<h2><span class="header-section-number">4.5</span> Building models</h2>
<p><br></p>
<p>Let’s build our first model trying to understand the Elantra sales vs the rest of the variables except for the month (you will see later how does the month affect the model).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MODEL 1</span>
ElantraLM =<span class="st"> </span><span class="kw">lm</span>(ElantraSales ~<span class="st"> </span>Unemployment +<span class="st"> </span>Queries +<span class="st"> </span>CPI_energy +<span class="st"> </span>CPI_all, <span class="dt">data=</span>ElantraTrain)
<span class="kw">summary</span>(ElantraLM)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ElantraSales ~ Unemployment + Queries + CPI_energy + 
##     CPI_all, data = ElantraTrain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6785.2 -2101.8  -562.5  2901.7  7021.0 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   95385.36  170663.81   0.559    0.580
## Unemployment  -3179.90    3610.26  -0.881    0.385
## Queries          19.03      11.26   1.690    0.101
## CPI_energy       38.51     109.60   0.351    0.728
## CPI_all        -297.65     704.84  -0.422    0.676
## 
## Residual standard error: 3295 on 31 degrees of freedom
## Multiple R-squared:  0.4282, Adjusted R-squared:  0.3544 
## F-statistic: 5.803 on 4 and 31 DF,  p-value: 0.00132</code></pre>
<p>This linear model is not very accurate (R-squared around 0.35) and there are no significant variables!! We definitely need to improve the linear model. In this case, we will add the variable Month to the model. Intuitively, we would think seasonality, in other words, depending on the month of the year, car sales must differ. For example, it is not the same if people have spent lots on Monday on Christmas holidays or not.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MODEL 2</span>
ElantraLM =<span class="st"> </span><span class="kw">lm</span>(ElantraSales ~<span class="st"> </span>Unemployment +<span class="st"> </span>Queries +<span class="st"> </span>CPI_energy +<span class="st"> </span>CPI_all +<span class="st"> </span>Month, <span class="dt">data=</span>ElantraTrain)
<span class="kw">summary</span>(ElantraLM)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ElantraSales ~ Unemployment + Queries + CPI_energy + 
##     CPI_all + Month, data = ElantraTrain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6416.6 -2068.7  -597.1  2616.3  7183.2 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  148330.49  195373.51   0.759   0.4536  
## Unemployment  -4137.28    4008.56  -1.032   0.3103  
## Queries          21.19      11.98   1.769   0.0871 .
## CPI_energy       54.18     114.08   0.475   0.6382  
## CPI_all        -517.99     808.26  -0.641   0.5265  
## Month           110.69     191.66   0.578   0.5679  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3331 on 30 degrees of freedom
## Multiple R-squared:  0.4344, Adjusted R-squared:  0.3402 
## F-statistic: 4.609 on 5 and 30 DF,  p-value: 0.003078</code></pre>
<p>But then, surprisingly, our model’s accuracy has decreased even though we have added a new variable to the previous model! But still our logic would tell us that seasonality must play a big part on the sales of certain products. What can be happening? The answer is that we have analysed the model + the month incorrectly. <strong>We have been analysing the month as an INTEGER variable.</strong> However, the month should be treated as a factor! By modelling Month as a factor variable, the effect of each calendar month is not restricted to be linear in the numerical coding of the month.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># CHANGING THE MONTH TO A FACTOR VARIABLE</span>
ElantraTrain$MonthFactor =<span class="st"> </span><span class="kw">as.factor</span>(ElantraTrain$Month)
ElantraTest$MonthFactor =<span class="st"> </span><span class="kw">as.factor</span>(ElantraTest$Month)

<span class="co"># RE-RUNNING MODEL 2 WITH MONTHFACTOR</span>
ElantraLM =<span class="st"> </span><span class="kw">lm</span>(ElantraSales ~<span class="st"> </span>Unemployment +<span class="st"> </span>Queries +<span class="st"> </span>CPI_energy +<span class="st"> </span>CPI_all +<span class="st"> </span>MonthFactor, <span class="dt">data=</span>ElantraTrain)
<span class="kw">summary</span>(ElantraLM)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ElantraSales ~ Unemployment + Queries + CPI_energy + 
##     CPI_all + MonthFactor, data = ElantraTrain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3865.1 -1211.7   -77.1  1207.5  3562.2 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   312509.280 144061.867   2.169 0.042288 *  
## Unemployment   -7739.381   2968.747  -2.607 0.016871 *  
## Queries           -4.764     12.938  -0.368 0.716598    
## CPI_energy       288.631     97.974   2.946 0.007988 ** 
## CPI_all        -1343.307    592.919  -2.266 0.034732 *  
## MonthFactor2    2254.998   1943.249   1.160 0.259540    
## MonthFactor3    6696.557   1991.635   3.362 0.003099 ** 
## MonthFactor4    7556.607   2038.022   3.708 0.001392 ** 
## MonthFactor5    7420.249   1950.139   3.805 0.001110 ** 
## MonthFactor6    9215.833   1995.230   4.619 0.000166 ***
## MonthFactor7    9929.464   2238.800   4.435 0.000254 ***
## MonthFactor8    7939.447   2064.629   3.845 0.001010 ** 
## MonthFactor9    5013.287   2010.745   2.493 0.021542 *  
## MonthFactor10   2500.184   2084.057   1.200 0.244286    
## MonthFactor11   3238.932   2397.231   1.351 0.191747    
## MonthFactor12   5293.911   2228.310   2.376 0.027621 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2306 on 20 degrees of freedom
## Multiple R-squared:  0.8193, Adjusted R-squared:  0.6837 
## F-statistic: 6.044 on 15 and 20 DF,  p-value: 0.0001469</code></pre>
<p>Aha! Now our model has definitely increased it’s accuracy (R-square around 0.68) and we do see there are certain months that do affect Elantra sales much more than others! Given this is the case, we might want to eliminate those non-significant variables to simplify the model as much as possible. We want to do this to avoid collinearity (there is a possibility that collinearity exists with this model as they has been changes in the coefficients of certain variables depending if we used month as INT or as FACTOR). In this case we will only remove the variable QUERIES as it is not significant. In addition, even though there are certain months that are not significant, we don’t want to remove the Month variable, so we will keep it in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># SIMPLIFYING THE MODEL</span>
ElantraLM =<span class="st"> </span><span class="kw">lm</span>(ElantraSales ~<span class="st"> </span>Unemployment +<span class="st"> </span>CPI_energy +<span class="st"> </span>CPI_all +<span class="st"> </span>MonthFactor, <span class="dt">data=</span>ElantraTrain)
<span class="kw">summary</span>(ElantraLM)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ElantraSales ~ Unemployment + CPI_energy + CPI_all + 
##     MonthFactor, data = ElantraTrain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3866.0 -1283.3  -107.2  1098.3  3650.1 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   325709.15  136627.85   2.384 0.026644 *  
## Unemployment   -7971.34    2840.79  -2.806 0.010586 *  
## CPI_energy       268.03      78.75   3.403 0.002676 ** 
## CPI_all        -1377.58     573.39  -2.403 0.025610 *  
## MonthFactor2    2410.91    1857.10   1.298 0.208292    
## MonthFactor3    6880.09    1888.15   3.644 0.001517 ** 
## MonthFactor4    7697.36    1960.21   3.927 0.000774 ***
## MonthFactor5    7444.64    1908.48   3.901 0.000823 ***
## MonthFactor6    9223.13    1953.64   4.721 0.000116 ***
## MonthFactor7    9602.72    2012.66   4.771 0.000103 ***
## MonthFactor8    7919.50    2020.99   3.919 0.000789 ***
## MonthFactor9    5074.29    1962.23   2.586 0.017237 *  
## MonthFactor10   2724.24    1951.78   1.396 0.177366    
## MonthFactor11   3665.08    2055.66   1.783 0.089062 .  
## MonthFactor12   5643.19    1974.36   2.858 0.009413 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2258 on 21 degrees of freedom
## Multiple R-squared:  0.818,  Adjusted R-squared:  0.6967 
## F-statistic: 6.744 on 14 and 21 DF,  p-value: 5.73e-05</code></pre>
<p><br></p>
</div>
<div id="predictions-on-train-and-test-set" class="section level2">
<h2><span class="header-section-number">4.6</span> Predictions on train and test set</h2>
<p><br></p>
<p>We have analysed the R-squared metric for this last model (~0.7) and we can check the residuals have been decreasing with the iterations we have done in each model. The next step is to predict, using the model formula, the results on the train and the test set. The train will be used as a proxy, but it is the test set we are interested in as this is unseen data for the model. In fact, we could find, given the training of the data was done before 2012 and the testing data represents 2012 and after, that our variables are not good enough if 2012 and after have a massively different market behaviour (lets say for example a market crash). Let’s check this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># PREDICTIONS ON TRAIN SET</span>
ElantraTrain$predictTrain =<span class="st"> </span><span class="kw">predict</span>(ElantraLM, <span class="dt">newdata =</span> ElantraTrain)
ElantraTrain$abs_error =<span class="st"> </span><span class="kw">abs</span>(ElantraTrain$ElantraSales -<span class="st"> </span>ElantraTrain$predictTrain)

<span class="co"># PREDICTIONS ON TEST SET</span>
ElantraTest$predictTest =<span class="st"> </span><span class="kw">predict</span>(ElantraLM, <span class="dt">newdata =</span> ElantraTest)
ElantraTest$abs_error =<span class="st"> </span><span class="kw">abs</span>(ElantraTest$ElantraSales -<span class="st"> </span>ElantraTest$predictTest)

<span class="co"># Print root mean square error</span>
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;RMSE train data&#39;</span>,<span class="kw">rmse</span>(<span class="dt">actual =</span> ElantraTrain$ElantraSales, <span class="dt">predicted =</span> ElantraTrain$predictTrain)))</code></pre></div>
<pre><code>## [1] &quot;RMSE train data 1724.53621996789&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;RMSE test data&#39;</span>,<span class="kw">rmse</span>(<span class="dt">actual =</span> ElantraTest$ElantraSales, <span class="dt">predicted =</span> ElantraTest$predictTest)))</code></pre></div>
<pre><code>## [1] &quot;RMSE test data 3691.28072474622&quot;</code></pre>
<p><br></p>
</div>
<div id="plotting-densities" class="section level2">
<h2><span class="header-section-number">4.7</span> Plotting densities</h2>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">density</span>(ElantraTrain$ElantraSales))
<span class="kw">lines</span>(<span class="kw">density</span>(ElantraTrain$predictTrain), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-43-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">density</span>(ElantraTest$ElantraSales))
<span class="kw">lines</span>(<span class="kw">density</span>(ElantraTest$predictTest), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-44-1.png" /><!-- --></p>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">4.8</span> Conclusion</h2>
<p>We have managed to show here a possible end to end process of how to build a linear model: from checking all variables to simplifying and reviewing important metrics. As you can see from the final plots, this model is not very accurate. Consider that we only work with 50 records, which is not very useful, but the important bit here was to get a feel of the modelling structure.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
